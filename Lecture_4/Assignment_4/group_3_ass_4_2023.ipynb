{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyreadstat in c:\\users\\auri\\appdata\\roaming\\python\\python311\\site-packages (1.2.6)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyreadstat) (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadstat) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: savReaderWriter in c:\\users\\auri\\appdata\\roaming\\python\\python311\\site-packages (3.4.2)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Iterable' from 'collections' (C:\\ProgramData\\anaconda3\\Lib\\collections\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msavReaderWriter\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msav\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\savReaderWriter\\__init__.py:138\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mheader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msavReader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msavWriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msavHeaderReader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msavReaderNp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\savReaderWriter\\savWriter.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlocale\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Iterable\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     pandasOK \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Iterable' from 'collections' (C:\\ProgramData\\anaconda3\\Lib\\collections\\__init__.py)"
     ]
    }
   ],
   "source": [
    "!pip install pyreadstat\n",
    "!pip install savReaderWriter\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import savReaderWriter as sav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their variables and values labels from this path `\"../../_data/endes/2019\"`. The name of imported files should be named as `rec_1`, `rec_2` and `rec_3` for files `REC0111.sav`, `RE223132.sav` and `RE516171.sav` respectively. The name of the variable and value labels should be `var_labels1` and `value_labels1` for `rec1`, `var_labels2` and `value_labels2` for `rec2`, and `var_labels3` and `value_labels3` for `rec3`. **Hint: See the section 3.3.4 of [the lecture 3](https://github.com/alexanderquispe/Diplomado_PUCP/blob/main/Lecture_3/Lecture_3.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_1 = pd.read_spss( r\"../../_data/endes/2019/REC0111.sav\" )\n",
    "rec_2 = pd.read_spss( r\"../../_data/endes/2019/RE223132.sav\" )\n",
    "rec_3 = pd.read_spss( r\"../../_data/endes/2019/RE516171.sav\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sav' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sav\u001b[38;5;241m.\u001b[39mSavHeaderReader( \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../_data/endes/2019/REC0111.sav\u001b[39m\u001b[38;5;124m\"\u001b[39m, ioUtf8\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m ) \u001b[38;5;28;01mas\u001b[39;00m header:\n\u001b[0;32m      2\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m header\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m      3\u001b[0m     value_labels1 \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mvalueLabels\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sav' is not defined"
     ]
    }
   ],
   "source": [
    "with sav.SavHeaderReader( r\"../../_data/endes/2019/REC0111.sav\", ioUtf8=True ) as header:\n",
    "    metadata = header.all()\n",
    "    value_labels1 = metadata.valueLabels\n",
    "    var_labels1 = metadata.varLabels\n",
    "\n",
    "with sav.SavHeaderReader( r\"../../_data/endes/2019/RE223132.sav\", ioUtf8=True ) as header:\n",
    "    metadata = header.all()\n",
    "    value_labels2 = metadata.valueLabels\n",
    "    var_labels2 = metadata.varLabels\n",
    "\n",
    "with sav.SavHeaderReader( r\"../../_data/endes/2019/RE516171.sav\", ioUtf8=True ) as header:\n",
    "    metadata = header.all()\n",
    "    value_labels3 = metadata.valueLabels\n",
    "    var_labels3 = metadata.varLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select the following columns for each data set:\n",
    "|Data|Columns|\n",
    "|---|---|\n",
    "|rec1| CASEID, V000, V001, V002, V003, V004, V007, V008, V009, V010, V011, V012, V024, V102, V120, V121, V122, V123, V124, V125, V127, V133 |\n",
    "|rec2| CASEID, V201, V218, V301, V302, V323, V323A, V325A, V326, V327, V337, V359, V360, V361, V362, V363, V364, V367, V372, V372A, V375A, V376, V376A, V379, V380 |\n",
    "|rec3| CASEID, V501, V502, V503, V504, V505, V506, V507, V508, V509, V510, V511, V512, V513, V525, V613, V714, V715 |\n",
    "\n",
    "\n",
    "Additioanlly, you should update the variables and value labels objects. They must have information only for the selected columns. The new dataframes should be name as `rec1_1`, `rec2_1`, and `rec3_1`. The new varible labels objects should be named as `new_var_labels1`, `new_var_labels2`, and `new_var_labels3`. The new value labels objects should be named as `new_value_labels1`, `new_value_labels2`, and `new_value_labels3` **Hint: Use the `loc` and column names to filter, `for loop`,   and [this link](https://stackoverflow.com/questions/3420122/filter-dict-to-contain-only-certain-keys) to update the var and value dictionary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_vars1 = \"CASEID, V000, V001, V002, V003, V004, V008, V009, V010, V011, V012, V024, V102, V120, V121, V122, V123, V124, V125, V127, V133\".split( ', ' )\n",
    "sel_vars2 = \"CASEID, V201, V218, V301, V302, V323, V323A, V325A, V326, V327, V337, V359, V360, V361, V362, V363, V364, V367, V372, V372A, V375A, V376, V376A, V379, V380\".split(', ')\n",
    "sel_vars3 = \"CASEID, V501, V502, V503, V504, V505, V506, V507, V508, V509, V510, V511, V512, V513, V525, V613, V714, V715\".split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rec_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rec1_1 \u001b[38;5;241m=\u001b[39m rec_1\u001b[38;5;241m.\u001b[39mloc[ :, sel_vars1 ]\n\u001b[0;32m      2\u001b[0m rec2_1 \u001b[38;5;241m=\u001b[39m rec_2\u001b[38;5;241m.\u001b[39mloc[ :, sel_vars2 ]\n\u001b[0;32m      3\u001b[0m rec3_1 \u001b[38;5;241m=\u001b[39m rec_3\u001b[38;5;241m.\u001b[39mloc[ :, sel_vars3 ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rec_1' is not defined"
     ]
    }
   ],
   "source": [
    "rec1_1 = rec_1.loc[ :, sel_vars1 ]\n",
    "rec2_1 = rec_2.loc[ :, sel_vars2 ]\n",
    "rec3_1 = rec_3.loc[ :, sel_vars3 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'value_labels1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m new_value_labels1 \u001b[38;5;241m=\u001b[39m { new_key: value_labels1[ new_key ] \u001b[38;5;28;01mfor\u001b[39;00m new_key \u001b[38;5;129;01min\u001b[39;00m sel_vars1 \u001b[38;5;28;01mif\u001b[39;00m new_key \u001b[38;5;129;01min\u001b[39;00m value_labels1\u001b[38;5;241m.\u001b[39mkeys() }\n\u001b[0;32m      2\u001b[0m new_var_labels1 \u001b[38;5;241m=\u001b[39m { new_key: var_labels1[ new_key ] \u001b[38;5;28;01mfor\u001b[39;00m new_key \u001b[38;5;129;01min\u001b[39;00m sel_vars1 }\n\u001b[0;32m      3\u001b[0m new_value_labels2 \u001b[38;5;241m=\u001b[39m { new_key: value_labels2[ new_key ] \u001b[38;5;28;01mfor\u001b[39;00m new_key \u001b[38;5;129;01min\u001b[39;00m sel_vars2 \u001b[38;5;28;01mif\u001b[39;00m new_key \u001b[38;5;129;01min\u001b[39;00m value_labels2\u001b[38;5;241m.\u001b[39mkeys() }\n",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m new_value_labels1 \u001b[38;5;241m=\u001b[39m { new_key: value_labels1[ new_key ] \u001b[38;5;28;01mfor\u001b[39;00m new_key \u001b[38;5;129;01min\u001b[39;00m sel_vars1 \u001b[38;5;28;01mif\u001b[39;00m new_key \u001b[38;5;129;01min\u001b[39;00m value_labels1\u001b[38;5;241m.\u001b[39mkeys() }\n\u001b[0;32m      2\u001b[0m new_var_labels1 \u001b[38;5;241m=\u001b[39m { new_key: var_labels1[ new_key ] \u001b[38;5;28;01mfor\u001b[39;00m new_key \u001b[38;5;129;01min\u001b[39;00m sel_vars1 }\n\u001b[0;32m      3\u001b[0m new_value_labels2 \u001b[38;5;241m=\u001b[39m { new_key: value_labels2[ new_key ] \u001b[38;5;28;01mfor\u001b[39;00m new_key \u001b[38;5;129;01min\u001b[39;00m sel_vars2 \u001b[38;5;28;01mif\u001b[39;00m new_key \u001b[38;5;129;01min\u001b[39;00m value_labels2\u001b[38;5;241m.\u001b[39mkeys() }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'value_labels1' is not defined"
     ]
    }
   ],
   "source": [
    "new_value_labels1 = { new_key: value_labels1[ new_key ] for new_key in sel_vars1 if new_key in value_labels1.keys() }\n",
    "new_var_labels1 = { new_key: var_labels1[ new_key ] for new_key in sel_vars1 }\n",
    "new_value_labels2 = { new_key: value_labels2[ new_key ] for new_key in sel_vars2 if new_key in value_labels2.keys() }\n",
    "new_var_labels2 = { new_key: var_labels2[ new_key ] for new_key in sel_vars2 }\n",
    "new_value_labels3 = { new_key: value_labels3[ new_key ] for new_key in sel_vars3 if new_key in value_labels3.keys() }\n",
    "new_var_labels3 = { new_key: var_labels3[ new_key ] for new_key in sel_vars3 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate a new column for `rec1_1` named as `year`. It should be equal to `2019`. Also, you must update this new variable for the `var_labels` dictionary. Generate a new key for `new_var_labels1` and the value for this key should be **\"Year of the survey\"** **Hint: Use `loc` and `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec1_1.loc[ :, \"year\" ] = np.array( [ 2019 ] * len( rec1_1 ) )\n",
    "new_var_labels1.update( { 'year': \"Year of the survey\" } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Merge `rec1_1`, `rec2_1`, and `rec3_1` using **CASEID**. Name this new object as `endes_2019`. **Hint: Use [this link](https://stackoverflow.com/questions/53645882/pandas-merging-101)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endes_20191 = rec1_1.merge( rec2_1, on = 'CASEID' ).merge( rec3_1, on = 'CASEID' )\n",
    "# endes_20192 = rec1_1.merge( rec2_1, on = 'CASEID', how = 'outer' ).merge( rec3_1, on = 'CASEID', how = 'outer' )\n",
    "# endes_2019.shape == endes_20192.shape <- # Output is True\n",
    "\n",
    "endes_2019 = rec1_1.merge( rec2_1, on = 'CASEID', how = 'outer' ).merge( rec3_1, on = 'CASEID', how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Unify all the `new_var_labels` in one object and `new_value_labels` in another one object. Name these two objects as `var_labels` and `value_labels`. Use them to generate new attributes for `endes_2019`. These attributes should be named as `var_labels` and `value_labels`. **Hint: Use `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = urllib.request.urlopen( r'https://www.dropbox.com/s/gwcssinb1j9zr6s/endes_2019_ta.pkl?dl=1' )\n",
    "endes_2019_ta = pickle.load( output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_labels = {}\n",
    "var_labels.update(new_var_labels1)\n",
    "var_labels.update(new_var_labels2)\n",
    "var_labels.update(new_var_labels3)\n",
    "\n",
    "value_labels = {}\n",
    "value_labels.update(new_value_labels1)\n",
    "value_labels.update(new_value_labels2)\n",
    "value_labels.update(new_value_labels3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endes_2019.attrs[ 'var_labels' ] = var_labels\n",
    "endes_2019.attrs[ 'value_labels' ] = value_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set1 = set( endes_2019.attrs[ 'var_labels' ] )\n",
    "# set2 = set( endes_2019_ta.attrs[ 'var_labels' ] )\n",
    "# set1 ^ set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endes_2019.attrs[ 'var_labels' ] == endes_2019_ta.attrs[ 'var_labels' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endes_2019.attrs[ 'value_labels' ] == endes_2019_ta.attrs[ 'value_labels' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now, replicate your code of the prevoius sections but for years **2019, 2018, 2017, 2016, 2015**. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their **variables and values labels** from this path `\"../../_data/endes/\"`. For this excersie you must use a for loop. This loop must iterate over **2019, 2018, 2017, 2016, 2015 folders** and import these files. All the files have the same name. You must store these files and their labels in a nested dictionary named as `all_data`. The keys of the dictionary should be named as `year_2019`, for example, and the keys of the nested dictionary should be `data`, `var_labels`, and `value_labels`. **Hint: Use [this link](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=4d6de78e00e7001f16bf6473c2eb7ce24fb611cd&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f616c6578616e6465727175697370652f4469706c6f6d61646f5f505543502f346436646537386530306537303031663136626636343733633265623763653234666236313163642f4c6563747572655f342f4c6563747572655f342e6970796e62&logged_in=true&nwo=alexanderquispe%2FDiplomado_PUCP&path=Lecture_4%2FLecture_4.ipynb&platform=windows&repository_id=427747212&repository_type=Repository&version=95#4.2.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_vars1 = \"CASEID, V000, V001, V002, V003, V004, V008, V009, V010, V011, V012, V024, V102, V120, V121, V122, V123, V124, V125, V127, V133\".split( ', ' )\n",
    "years = [ '2015', '2016', '2017', '2018', '2019' ]\n",
    "all_data = {}\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    data_1 = pd.read_spss( f\"../../_data/endes/{year}/REC0111.sav\" )\n",
    "    data_2 = pd.read_spss( f\"../../_data/endes/{year}/RE223132.sav\" )\n",
    "    data_3 = pd.read_spss( f\"../../_data/endes/{year}/RE516171.sav\" )\n",
    "    \n",
    "    with sav.SavHeaderReader( f\"../../_data/endes/{year}/REC0111.sav\", ioUtf8=True ) as header:\n",
    "        metadata = header.all()\n",
    "        value_labels1 = metadata.valueLabels\n",
    "        var_labels1 = metadata.varLabels\n",
    "\n",
    "    with sav.SavHeaderReader( f\"../../_data/endes/{year}/RE223132.sav\", ioUtf8=True ) as header:\n",
    "        metadata = header.all()\n",
    "        value_labels2 = metadata.valueLabels\n",
    "        var_labels2 = metadata.varLabels\n",
    "\n",
    "    with sav.SavHeaderReader( f\"../../_data/endes/{year}/RE516171.sav\", ioUtf8=True ) as header:\n",
    "        metadata = header.all()\n",
    "        value_labels3 = metadata.valueLabels\n",
    "        var_labels3 = metadata.varLabels\n",
    "    \n",
    "    data1_1 = data_1.loc[ :, sel_vars1 ]\n",
    "    data2_1 = data_2.loc[ :, sel_vars2 ]\n",
    "    data3_1 = data_3.loc[ :, sel_vars3 ]\n",
    "    \n",
    "    new_value_labels1 = { new_key: value_labels1[ new_key ] for new_key in sel_vars1 if new_key in value_labels1.keys() }\n",
    "    new_var_labels1 = { new_key: var_labels1[ new_key ] for new_key in sel_vars1 }\n",
    "    new_value_labels2 = { new_key: value_labels2[ new_key ] for new_key in sel_vars2 if new_key in value_labels2.keys() }\n",
    "    new_var_labels2 = { new_key: var_labels2[ new_key ] for new_key in sel_vars2 }\n",
    "    new_value_labels3 = { new_key: value_labels3[ new_key ] for new_key in sel_vars3 if new_key in value_labels3.keys() }\n",
    "    new_var_labels3 = { new_key: var_labels3[ new_key ] for new_key in sel_vars3 }\n",
    "    \n",
    "    data1_1.loc[ :, \"year\" ] = np.array( [ int( year ) ] * len( data1_1 ) )\n",
    "    new_var_labels1.update( { 'year': \"Year of the survey\" } )\n",
    "    \n",
    "    data = data1_1.merge( data2_1, on = 'CASEID' ).merge( data3_1, on = 'CASEID' )\n",
    "    \n",
    "    var_labels = {}\n",
    "    var_labels.update( new_var_labels1 )\n",
    "    var_labels.update( new_var_labels2 )\n",
    "    var_labels.update( new_var_labels3 )\n",
    "\n",
    "    value_labels = {}\n",
    "    value_labels.update( new_value_labels1 )\n",
    "    value_labels.update( new_value_labels2 )\n",
    "    value_labels.update( new_value_labels3 )\n",
    "    \n",
    "    all_data.update( { f'year_{ year }': { 'data': data, 'var_labels': var_labels, 'value_labels': value_labels } } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use `all_data` to append all the data sets. Store all data sets in a list using `for loop`. Then, use `pd.concat` to append all the data sets. Also, you must reset the index to have a good-looking data. This new object should be named as `endes_data_2015_2019`. **Hint: Use [this code](https://stackoverflow.com/questions/32444138/concatenate-a-list-of-pandas-dataframes-together)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First of all, we create a list that stores the entire data set\n",
    "all_datasets = []\n",
    "\n",
    "#Then we iterate every year.\n",
    "for year_key in all_data.keys():\n",
    "\n",
    "#We get the _merged data for the current year    \n",
    "    merged_data = all_data[year_key].get('merged_data')\n",
    "    \n",
    " #check if merged_data is none   \n",
    "     if merged_data is not None:\n",
    " #paste the merged data to the list       \n",
    "            all_datasets.append(merged_data)\n",
    " #put all the data together in a single dataframe   \n",
    "            endes_data_2015_2019 = pd.concat(all_datasets, ignore_index=True)\n",
    " #we reset the index for the dataframe   \n",
    "            endes_data_2015_2019.reset_index(drop=True, inplace=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endes_data_2015_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Store all the `var_labels` and `value_labels` in a dictionary named as `all_var_labels` and `all_value_labels`. The first keys should be the year for both dictionaries.Then, use them to generate new attributes for `endes_data_2015_2019`. These attributes should be named as `var_labels` and `value_labels`.  **Hint: Use [this link](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=4d6de78e00e7001f16bf6473c2eb7ce24fb611cd&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f616c6578616e6465727175697370652f4469706c6f6d61646f5f505543502f346436646537386530306537303031663136626636343733633265623763653234666236313163642f4c6563747572655f342f4c6563747572655f342e6970796e62&logged_in=true&nwo=alexanderquispe%2FDiplomado_PUCP&path=Lecture_4%2FLecture_4.ipynb&platform=windows&repository_id=427747212&repository_type=Repository&version=95#4.2.3.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First of all we must create two dictionaries\n",
    "all_var_labels = {}\n",
    "all_value_labels = {}\n",
    "\n",
    "#Then, a loop is used to iterate over the period of years\n",
    "#Now, we will extract the labels of the variables and their value in each year\n",
    "\n",
    "for year in range( 2015, 2020 ):\n",
    "    all_var_labels[year] = all_data[ f'year_{year}' ]['var_labels']\n",
    "    all_value_labels[year] = all_data[ f'year_{year}' ]['value_labels']\n",
    "\n",
    "all_var_labels\n",
    "all_value_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we establish the var_label and value_labels properties, to which the information from the previously defined dictionaries is assigned.\n",
    "endes_data_2015_2019_ta.attrs[ 'var_labels' ] = all_var_labels\n",
    "endes_data_2015_2019_ta.attrs[ 'value_labels' ] = all_value_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Use `endes_data_2015_2019` data to generate a new object named `mean_key_vars` to find the mean of **total children ever born (V201)**, **Ideal number of children (V613)**, **Husbands education-single yrs (V715)**, and **Age at first marriage (V511)** by year and department **(V024)**. Name these columns as **mean_total_children, mean_ideal_children, mean_hb_yr_educ and mean_first_marriage**, respectively. **Hint: Use groupby and [this link](https://stackoverflow.com/questions/40901770/is-there-a-simple-way-to-change-a-column-of-yes-no-to-1-0-in-a-pandas-dataframe).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AgrupamoWe group the data by year and department and calculate the mean.s los datos por año y por departamento y calcula la media\n",
    "endes_data_2015_2019_ta=endes_data_2015_2019[[ 'V201', 'V613', 'V715', 'V511','V024','year']]\n",
    "mean_key_vars = endes_data_2015_2019_ta.groupby(['V024', 'year'])[['V201', 'V613', 'V715', 'V511']].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Rename the columns.\n",
    " mean_key_vars.columns = ['mean_total_children', 'mean_ideal_children', 'mean_hb_yr_educ', 'mean_first_marriage']\n",
    "\n",
    "    return mean_key_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Merge `mean_key_vars` with `endes_data_2015_2019`. Name this object `final_result`. **Hint: Use merge.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We combine the two data objects into one, using year and V204\n",
    "    final_result = pd.merge(endes_data, mean_key_vars, on=['V024', 'year'])\n",
    "\n",
    "#  Rename the columns. \n",
    "    final_result.columns = ['mean_total_children', 'mean_ideal_children', 'mean_hb_yr_educ', 'mean_first_marriage']\n",
    "\n",
    "    return final_result"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
