{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises:\n",
    "\n",
    "- Adriana: 1 and 2\n",
    "- Alejandro: 3 and 4\n",
    "- Mauricio: 5 and 6\n",
    "- André: 7\n",
    "- Alexsander: 8 and 9\n",
    "- Karla: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyreadstat\n",
      "  Obtaining dependency information for pyreadstat from https://files.pythonhosted.org/packages/37/1d/25de8cc1925668685d25ffba1c79b4a77faab3ef0a571b40e8a30bf5aba4/pyreadstat-1.2.7-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pyreadstat-1.2.7-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\adriana\\anaconda3\\lib\\site-packages (from pyreadstat) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\adriana\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adriana\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\adriana\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adriana\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyreadstat) (1.16.0)\n",
      "Downloading pyreadstat-1.2.7-cp311-cp311-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.4 MB 980.4 kB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.2/2.4 MB 1.0 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.2/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.3/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/2.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/2.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/2.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.5/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.5/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.5/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.6/2.4 MB 964.0 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.6/2.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.7/2.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.4 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.8/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 1.0/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.0/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.0/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.2/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.2/2.4 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.3/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.4/2.4 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.6/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.9/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.0/2.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.3/2.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.3/2.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pyreadstat\n",
      "Successfully installed pyreadstat-1.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pyreadstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their variables and values labels from this path `\"../../_data/endes/2019\"`. The name of imported files should be named as `rec_1`, `rec_2` and `rec_3` for files `REC0111.sav`, `RE223132.sav` and `RE516171.sav` respectively. The name of the variable and value labels should be `var_labels1` and `value_labels1` for `rec1`, `var_labels2` and `value_labels2` for `rec2`, and `var_labels3` and `value_labels3` for `rec3`. **Hint: See the section 3.3.4 of [the lecture 3](https://github.com/alexanderquispe/Diplomado_PUCP/blob/main/Lecture_3/Lecture_3.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38335, 105)\n",
      "(36922, 176)\n",
      "(33311, 84)\n"
     ]
    }
   ],
   "source": [
    "file_paths = [\n",
    "    \"../../_data/endes/2019/REC0111.sav\",\n",
    "    \"../../_data/endes/2019/RE223132.sav\",\n",
    "    \"../../_data/endes/2019/RE516171.sav\"] #we import the files with the path\n",
    "\n",
    "#we download data and labels and we return the df\n",
    "data_frames = {}\n",
    "var_labels = {}\n",
    "value_labels = {}\n",
    "\n",
    "def load_sav_file(file_path):\n",
    "    df, meta = pyreadstat.read_sav(file_path)\n",
    "    var_label_dict = dict(zip(df.columns, meta.column_labels))  # We turn into dictionaries\n",
    "    return df, var_label_dict, meta.value_labels\n",
    "\n",
    "#names of files\n",
    "names = [\"rec_1\", \"rec_2\", \"rec_3\"]\n",
    "\n",
    "#we zip the files and create the df\n",
    "for name, path in zip(names, file_paths):\n",
    "    df, var_label, value_label = load_sav_file(path)\n",
    "    data_frames[name] = df\n",
    "    var_labels[f'var_labels{name[-1]}'] = var_label\n",
    "    value_labels[f'value_labels{name[-1]}'] = value_label\n",
    "\n",
    "#new files\n",
    "rec_1 = data_frames[\"rec_1\"]\n",
    "rec_2 = data_frames[\"rec_2\"]\n",
    "rec_3 = data_frames[\"rec_3\"]\n",
    "\n",
    "#names of variables and value labels\n",
    "var_labels1 = var_labels[\"var_labels1\"]\n",
    "value_labels1 = value_labels[\"value_labels1\"]\n",
    "\n",
    "var_labels2 = var_labels[\"var_labels2\"]\n",
    "value_labels2 = value_labels[\"value_labels2\"]\n",
    "\n",
    "var_labels3 = var_labels[\"var_labels3\"]\n",
    "value_labels3 = value_labels[\"value_labels3\"]\n",
    "\n",
    "# Let's see the shape (rows, columns) of the dataframes\n",
    "print(rec_1.shape)\n",
    "print(rec_2.shape)\n",
    "print(rec_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38335, 105)\n",
      "(36922, 176)\n",
      "(33311, 84)\n"
     ]
    }
   ],
   "source": [
    "#### [ESTA ES UNA VERSIÓN ALTERNATIVA DEL CÓDIGO ANTERIOR]\n",
    "\n",
    "# To import files with pyreadstat, capturing both the dataframe and metadata,\n",
    " # We can bypass the initial file_path going directly to the pyreadstat\n",
    "rec_1, meta1 = pyreadstat.read_sav(\"../../_data/endes/2019/REC0111.sav\")\n",
    "rec_2, meta2 = pyreadstat.read_sav(\"../../_data/endes/2019/RE223132.sav\")\n",
    "rec_3, meta3 = pyreadstat.read_sav(\"../../_data/endes/2019/RE516171.sav\")\n",
    "\n",
    "# Then, we extract variable and value labels\n",
    "var_labels1 = meta1.column_labels\n",
    "value_labels1 = meta1.value_labels\n",
    "\n",
    "var_labels2 = meta2.column_labels\n",
    "value_labels2 = meta2.value_labels\n",
    "\n",
    "var_labels3 = meta3.column_labels\n",
    "value_labels3 = meta3.value_labels\n",
    "\n",
    "# Let's see the shape (rows, columns) of the dataframes\n",
    "print(rec_1.shape)\n",
    "print(rec_2.shape)\n",
    "print(rec_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select the following columns for each data set. Check if all the columns are in the dataset. Make a code that check the columns that are not included. Please, reporte them.\n",
    "\n",
    "|Data|Columns|\n",
    "|---|---|\n",
    "|rec1| CASEID, V000, V001, V002, V003, V004, V007, V008, V009, V010, V011, V012, V024, V102, V120, V121, V122, V123, V124, V125, V127, V133 |\n",
    "|rec2| CASEID, V201, V218, V301, V302, V323, V323A, V325A, V326, V327, V337, V359, V360, V361, V362, V363, V364, V367, V372, V372A, V375A, V376, V376A, V379, V380 |\n",
    "|rec3| CASEID, V501, V502, V503, V504, V505, V506, V507, V508, V509, V510, V511, V512, V513, V525, V613, V714, V715 |\n",
    "\n",
    "\n",
    "Additioanlly, you should update the variables and value labels objects. They must have information only for the selected columns. The new dataframes should be name as `rec1_1`, `rec2_1`, and `rec3_1`. The new varible labels objects should be named as `new_var_labels1`, `new_var_labels2`, and `new_var_labels3`. The new value labels objects should be named as `new_value_labels1`, `new_value_labels2`, and `new_value_labels3` **Hint: Use the `loc` and column names to filter, `for loop`,   and [this link](https://stackoverflow.com/questions/3420122/filter-dict-to-contain-only-certain-keys) to update the var and value dictionary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38335, 22)\n",
      "(36922, 24)\n",
      "(33311, 18)\n"
     ]
    }
   ],
   "source": [
    "#first, we select the columns\n",
    "columns_dict = {\n",
    "    \"rec_1\": [\"CASEID\", \"V000\", \"V001\", \"V002\", \"V003\", \"V004\", \"V007\", \"V008\", \"V009\", \"V010\", \"V011\", \"V012\", \"V024\", \"V102\", \"V120\", \"V121\", \"V122\", \"V123\", \"V124\", \"V125\", \"V127\", \"V133\"],\n",
    "    \"rec_2\": [\"CASEID\", \"V201\", \"V218\", \"V301\", \"V302\", \"V323\", \"V325A\", \"V326\", \"V327\", \"V337\", \"V359\", \"V360\", \"V361\", \"V362\", \"V363\", \"V364\", \"V367\", \"V372\", \"V372A\", \"V375A\", \"V376\", \"V376A\", \"V379\", \"V380\"],\n",
    "    \"rec_3\": [\"CASEID\", \"V501\", \"V502\", \"V503\", \"V504\", \"V505\", \"V506\", \"V507\", \"V508\", \"V509\", \"V510\", \"V511\", \"V512\", \"V513\", \"V525\", \"V613\", \"V714\", \"V715\"]}\n",
    "\n",
    "# We update variable labels\n",
    "def update_var_labels(var_labels, selected_columns):\n",
    "    return {k: v for k, v in var_labels.items() if k in selected_columns}\n",
    "\n",
    "# We update value labels\n",
    "def update_value_labels(value_labels, selected_columns):\n",
    "    new_value_labels = {}\n",
    "    for k, v in value_labels.items():\n",
    "        if k in selected_columns:\n",
    "            new_value_labels[k] = v\n",
    "    return new_value_labels\n",
    "\n",
    "# Diccionarios para almacenar los DataFrames filtrados y las nuevas etiquetas\n",
    "filtered_data_frames = {}\n",
    "new_var_labels = {}\n",
    "new_value_labels = {}\n",
    "\n",
    "# We use loc to filter dfs and update labels\n",
    "for name, df in data_frames.items():\n",
    "    columns = columns_dict[name]\n",
    "    filtered_df = df.loc[:, columns]\n",
    "    filtered_data_frames[f'{name}_1'] = filtered_df\n",
    "    new_var_labels[f'new_var_labels{name[-1]}'] = update_var_labels(var_labels[f'var_labels{name[-1]}'], columns)\n",
    "    new_value_labels[f'new_value_labels{name[-1]}'] = update_value_labels(value_labels[f'value_labels{name[-1]}'], columns)\n",
    "\n",
    "# We create the new dfs\n",
    "rec1_1 = filtered_data_frames[\"rec_1_1\"]\n",
    "rec2_1 = filtered_data_frames[\"rec_2_1\"]\n",
    "rec3_1 = filtered_data_frames[\"rec_3_1\"]\n",
    "\n",
    "new_var_labels1 = new_var_labels[\"new_var_labels1\"]\n",
    "new_value_labels1 = new_value_labels[\"new_value_labels1\"]\n",
    "\n",
    "new_var_labels2 = new_var_labels[\"new_var_labels2\"]\n",
    "new_value_labels2 = new_value_labels[\"new_value_labels2\"]\n",
    "\n",
    "new_var_labels3 = new_var_labels[\"new_var_labels3\"]\n",
    "new_value_labels3 = new_value_labels[\"new_value_labels3\"]\n",
    "\n",
    "# Checking the new shapes of our df\n",
    "print(rec1_1.shape)\n",
    "print(rec2_1.shape)\n",
    "print(rec3_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38335, 22)\n",
      "(36922, 25)\n",
      "(33311, 18)\n"
     ]
    }
   ],
   "source": [
    "######## [VERSIÓN ALTERNATIVA]\n",
    "\n",
    "# We make a list with the columns to select from each df\n",
    "cols_rec1 = [\"CASEID\", \"V000\", \"V001\", \"V002\", \"V003\", \"V004\", \"V007\", \"V008\", \"V009\", \"V010\", \"V011\", \"V012\", \"V024\", \"V102\", \"V120\", \"V121\", \"V122\", \"V123\", \"V124\", \"V125\", \"V127\", \"V133\"]\n",
    "cols_rec2 = [\"CASEID\", \"V201\", \"V218\", \"V301\", \"V302\", \"V323\", \"V323A\", \"V325A\", \"V326\", \"V327\", \"V337\", \"V359\", \"V360\", \"V361\", \"V362\", \"V363\", \"V364\", \"V367\", \"V372\", \"V372A\", \"V375A\", \"V376\", \"V376A\", \"V379\", \"V380\"]\n",
    "cols_rec3 = [\"CASEID\", \"V501\", \"V502\", \"V503\", \"V504\", \"V505\", \"V506\", \"V507\", \"V508\", \"V509\", \"V510\", \"V511\", \"V512\", \"V513\", \"V525\", \"V613\", \"V714\", \"V715\"]\n",
    "\n",
    "# We define a function to select and check for missing columns\n",
    "def check_and_select_cols(df, cols, dataset_name):\n",
    "    # Select columns that are present in the dataframe\n",
    "    selected_cols = [col for col in cols if col in df.columns]\n",
    "    # Identify columns that are missing from the dataframe\n",
    "    missing_cols = [col for col in cols if col not in df.columns]\n",
    "    \n",
    "    # If there are missing columns, print a message\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns for {dataset_name}: {missing_cols}\")\n",
    "    \n",
    "    # Return the dataframe with only the selected columns\n",
    "    return df.loc[:, selected_cols]\n",
    "\n",
    "# Applying function to each dataset\n",
    "rec1_1 = check_and_select_cols(rec_1, cols_rec1, \"rec1\")\n",
    "rec2_1 = check_and_select_cols(rec_2, cols_rec2, \"rec2\")\n",
    "rec3_1 = check_and_select_cols(rec_3, cols_rec3, \"rec3\")\n",
    "\n",
    "# We define a function to update variable labels for selected columns\n",
    "def update_labels(old_labels, selected_cols):\n",
    "    # Create a new dictionary with only the selected columns\n",
    "    updated_labels = {key: old_labels[key] for key in selected_cols if key in old_labels}\n",
    "    return updated_labels\n",
    "\n",
    "# Updating variable labels\n",
    "new_var_labels1 = update_labels(var_labels1, cols_rec1)\n",
    "new_var_labels2 = update_labels(var_labels2, cols_rec2)\n",
    "new_var_labels3 = update_labels(var_labels3, cols_rec3)\n",
    "\n",
    "# Function to update value labels for selected columns\n",
    "def update_value_labels(old_labels, selected_cols):\n",
    "    # Create a new dictionary to hold updated value labels\n",
    "    updated_labels = {key: old_labels[key] for key in selected_cols if key in old_labels}\n",
    "    return updated_labels\n",
    "\n",
    "# Updating value labels\n",
    "new_value_labels1 = update_value_labels(value_labels1, cols_rec1)\n",
    "new_value_labels2 = update_value_labels(value_labels2, cols_rec2)\n",
    "new_value_labels3 = update_value_labels(value_labels3, cols_rec3)\n",
    "\n",
    "# Checking the new shapes of our df\n",
    "print(rec1_1.shape)\n",
    "print(rec2_1.shape)\n",
    "print(rec3_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate a new column for `rec1_1` named as `year`. It should be equal to `2019`. Also, you must update this new variable for the `var_labels` dictionary. Generate a new key for `new_var_labels1` and the value for this key should be **\"Year of the survey\"** **Hint: Use `loc` and `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38335, 22)\n",
      "(38335, 23)\n",
      "{'year': 'Year of the survey'}\n"
     ]
    }
   ],
   "source": [
    "# To know the current shape (rows, columns) of the df\n",
    "print(rec1_1.shape)\n",
    "\n",
    "# We'll add the column 'year' through .loc(), all the rows (':') will have the same value (2019) \n",
    "rec1_1.loc[:, 'year'] = 2019\n",
    "\n",
    "# To verify that there's an extra column\n",
    "print(rec1_1.shape)\n",
    "\n",
    "# Next, we update the variable labels to include our new column.\n",
    " # Remember, it has to be like a dictionary because that's the kind of object new_var_labels1 is.\n",
    "new_var_labels1.update({'year': \"Year of the survey\"})\n",
    "\n",
    "print(new_var_labels1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Merge `rec1_1`, `rec2_1`, and `rec3_1` using **CASEID**. Name this new object as `endes_2019`. **Hint: Use [this link](https://stackoverflow.com/questions/53645882/pandas-merging-101)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38335, 64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASEID</th>\n",
       "      <th>V000</th>\n",
       "      <th>V001</th>\n",
       "      <th>V002</th>\n",
       "      <th>V003</th>\n",
       "      <th>V004</th>\n",
       "      <th>V007</th>\n",
       "      <th>V008</th>\n",
       "      <th>V009</th>\n",
       "      <th>V010</th>\n",
       "      <th>...</th>\n",
       "      <th>V508</th>\n",
       "      <th>V509</th>\n",
       "      <th>V510</th>\n",
       "      <th>V511</th>\n",
       "      <th>V512</th>\n",
       "      <th>V513</th>\n",
       "      <th>V525</th>\n",
       "      <th>V613</th>\n",
       "      <th>V714</th>\n",
       "      <th>V715</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000100201  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000100201  3</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000102801  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000102801  6</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000104801  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38330</th>\n",
       "      <td>325406201  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38331</th>\n",
       "      <td>325406301  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>1236.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38332</th>\n",
       "      <td>325407001  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38333</th>\n",
       "      <td>325407201  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38334</th>\n",
       "      <td>325407401  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1362.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38335 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   CASEID V000    V001  V002  V003    V004    V007    V008  \\\n",
       "0            000100201  2  PE6     1.0   2.0   2.0     1.0  2019.0  1434.0   \n",
       "1            000100201  3  PE6     1.0   2.0   3.0     1.0  2019.0  1434.0   \n",
       "2            000102801  2  PE6     1.0  28.0   2.0     1.0  2019.0  1434.0   \n",
       "3            000102801  6  PE6     1.0  28.0   6.0     1.0  2019.0  1434.0   \n",
       "4            000104801  2  PE6     1.0  48.0   2.0     1.0  2019.0  1434.0   \n",
       "...                   ...  ...     ...   ...   ...     ...     ...     ...   \n",
       "38330        325406201  2  PE6  3254.0  62.0   2.0  3254.0  2019.0  1440.0   \n",
       "38331        325406301  2  PE6  3254.0  63.0   2.0  3254.0  2019.0  1440.0   \n",
       "38332        325407001  2  PE6  3254.0  70.0   2.0  3254.0  2019.0  1440.0   \n",
       "38333        325407201  2  PE6  3254.0  72.0   2.0  3254.0  2019.0  1440.0   \n",
       "38334        325407401  2  PE6  3254.0  74.0   2.0  3254.0  2019.0  1440.0   \n",
       "\n",
       "       V009    V010  ...    V508    V509  V510  V511  V512  V513  V525  V613  \\\n",
       "0       4.0  1986.0  ...  2008.0  1297.0   1.0  21.0  11.0   3.0  17.0   2.0   \n",
       "1       1.0  2007.0  ...     NaN     NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2       6.0  1983.0  ...  2011.0  1344.0   1.0  28.0   7.0   2.0  18.0   3.0   \n",
       "3       3.0  1970.0  ...  1984.0  1016.0   5.0  14.0  34.0   7.0  14.0   0.0   \n",
       "4       5.0  1991.0  ...  2009.0  1320.0   1.0  18.0   9.0   2.0  15.0   2.0   \n",
       "...     ...     ...  ...     ...     ...   ...   ...   ...   ...   ...   ...   \n",
       "38330  12.0  1971.0  ...  1985.0  1032.0   1.0  14.0  34.0   7.0  12.0   3.0   \n",
       "38331   6.0  1988.0  ...  2002.0  1236.0   1.0  14.0  17.0   4.0  12.0   3.0   \n",
       "38332   7.0  1973.0  ...  1986.0  1043.0   1.0  13.0  33.0   7.0  12.0   4.0   \n",
       "38333  12.0  1994.0  ...  2008.0  1301.0   1.0  13.0  11.0   3.0  13.0   2.0   \n",
       "38334  10.0  1996.0  ...  2013.0  1362.0   1.0  16.0   6.0   2.0  15.0   2.0   \n",
       "\n",
       "       V714  V715  \n",
       "0       1.0  11.0  \n",
       "1       NaN   NaN  \n",
       "2       1.0  14.0  \n",
       "3       0.0   6.0  \n",
       "4       0.0   6.0  \n",
       "...     ...   ...  \n",
       "38330   0.0   5.0  \n",
       "38331   0.0   7.0  \n",
       "38332   0.0   5.0  \n",
       "38333   0.0  11.0  \n",
       "38334   0.0  11.0  \n",
       "\n",
       "[38335 rows x 64 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We merge the datasets using the 'CASEID' column as our common variable (on = 'CASEID') and indicating 'outer' as the method\n",
    " # By using 'outer' we make sure that all our selected variables are preserved in the new dataset\n",
    " # Because 'outer' employs the union (all variables), whereas 'inner' only considers the intersection (shared variables)\n",
    "endes_2019 = pd.merge(rec1_1, rec2_1, on = 'CASEID', how = 'outer')\n",
    "endes_2019 = pd.merge(endes_2019, rec3_1, on = 'CASEID', how = 'outer')\n",
    "\n",
    "# The df should have 64 columns, the sum of columns numbers in the composing df minus two (CASEID is present in all three fd, only gets counted once)\n",
    "print(endes_2019.shape)\n",
    "# Let's check if everything is in order\n",
    "endes_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Unify all the `new_var_labels` in one object and `new_value_labels` in another one object. Name these two objects as `var_labels` and `value_labels`. Use them to generate new attributes for `endes_2019`. These attributes should be named as `var_labels` and `value_labels`. **Hint: Use `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To combine all new_var_labels into a single dictionary\n",
    "var_labels = new_var_labels1.copy()\n",
    "var_labels.update(new_var_labels2)\n",
    "var_labels.update(new_var_labels3)\n",
    "\n",
    "# Now, we do the same for all new_value_labels\n",
    "value_labels = new_value_labels1.copy()\n",
    "value_labels.update(new_value_labels2)\n",
    "value_labels.update(new_value_labels3)\n",
    "\n",
    "# Finally, we add the new attributes to the endes_2019 dataframe\n",
    "endes_2019.attrs['var_labels'] = var_labels\n",
    "endes_2019.attrs['value_labels'] = value_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now, replicate your code of the prevoius sections but for years **2019, 2018, 2017, 2016, 2015**. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their **variables and values labels** from this path `\"../../_data/endes/\"`. For this excersie you must use a for loop. This loop must iterate over **2019, 2018, 2017, 2016, 2015 folders** and import these files. All the files have the same name. You must store these files and their labels in a nested dictionary named as `all_data`. The keys of the dictionary should be named as `year_2019`, for example, and the keys of the nested dictionary should be `data`, `var_labels`, and `value_labels`. **Hint: Use [this link](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=4d6de78e00e7001f16bf6473c2eb7ce24fb611cd&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f616c6578616e6465727175697370652f4469706c6f6d61646f5f505543502f346436646537386530306537303031663136626636343733633265623763653234666236313163642f4c6563747572655f342f4c6563747572655f342e6970796e62&logged_in=true&nwo=alexanderquispe%2FDiplomado_PUCP&path=Lecture_4%2FLecture_4.ipynb&platform=windows&repository_id=427747212&repository_type=Repository&version=95#4.2.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns for rec1 in year 2018: ['V007']\n",
      "{'year_2019': {'data':                    CASEID V000    V001  V002  V003    V004    V007    V008  \\\n",
      "0            000100201  2  PE6     1.0   2.0   2.0     1.0  2019.0  1434.0   \n",
      "1            000100201  3  PE6     1.0   2.0   3.0     1.0  2019.0  1434.0   \n",
      "2            000102801  2  PE6     1.0  28.0   2.0     1.0  2019.0  1434.0   \n",
      "3            000102801  6  PE6     1.0  28.0   6.0     1.0  2019.0  1434.0   \n",
      "4            000104801  2  PE6     1.0  48.0   2.0     1.0  2019.0  1434.0   \n",
      "...                   ...  ...     ...   ...   ...     ...     ...     ...   \n",
      "38330        325406201  2  PE6  3254.0  62.0   2.0  3254.0  2019.0  1440.0   \n",
      "38331        325406301  2  PE6  3254.0  63.0   2.0  3254.0  2019.0  1440.0   \n",
      "38332        325407001  2  PE6  3254.0  70.0   2.0  3254.0  2019.0  1440.0   \n",
      "38333        325407201  2  PE6  3254.0  72.0   2.0  3254.0  2019.0  1440.0   \n",
      "38334        325407401  2  PE6  3254.0  74.0   2.0  3254.0  2019.0  1440.0   \n",
      "\n",
      "       V009    V010  ...    V508    V509  V510  V511  V512  V513  V525  V613  \\\n",
      "0       4.0  1986.0  ...  2008.0  1297.0   1.0  21.0  11.0   3.0  17.0   2.0   \n",
      "1       1.0  2007.0  ...     NaN     NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "2       6.0  1983.0  ...  2011.0  1344.0   1.0  28.0   7.0   2.0  18.0   3.0   \n",
      "3       3.0  1970.0  ...  1984.0  1016.0   5.0  14.0  34.0   7.0  14.0   0.0   \n",
      "4       5.0  1991.0  ...  2009.0  1320.0   1.0  18.0   9.0   2.0  15.0   2.0   \n",
      "...     ...     ...  ...     ...     ...   ...   ...   ...   ...   ...   ...   \n",
      "38330  12.0  1971.0  ...  1985.0  1032.0   1.0  14.0  34.0   7.0  12.0   3.0   \n",
      "38331   6.0  1988.0  ...  2002.0  1236.0   1.0  14.0  17.0   4.0  12.0   3.0   \n",
      "38332   7.0  1973.0  ...  1986.0  1043.0   1.0  13.0  33.0   7.0  12.0   4.0   \n",
      "38333  12.0  1994.0  ...  2008.0  1301.0   1.0  13.0  11.0   3.0  13.0   2.0   \n",
      "38334  10.0  1996.0  ...  2013.0  1362.0   1.0  16.0   6.0   2.0  15.0   2.0   \n",
      "\n",
      "       V714  V715  \n",
      "0       1.0  11.0  \n",
      "1       NaN   NaN  \n",
      "2       1.0  14.0  \n",
      "3       0.0   6.0  \n",
      "4       0.0   6.0  \n",
      "...     ...   ...  \n",
      "38330   0.0   5.0  \n",
      "38331   0.0   7.0  \n",
      "38332   0.0   5.0  \n",
      "38333   0.0  11.0  \n",
      "38334   0.0  11.0  \n",
      "\n",
      "[38335 rows x 64 columns], 'var_labels': {'year': 'Year of the survey'}, 'value_labels': {}}, 'year_2018': {'data':                    CASEID V000    V001  V002  V003    V004    V008  V009  \\\n",
      "0            000100701  2  PE6     1.0   7.0   2.0     1.0  1422.0   8.0   \n",
      "1            000100701  3  PE6     1.0   7.0   3.0     1.0  1422.0   2.0   \n",
      "2            000101401  2  PE6     1.0  14.0   2.0     1.0  1422.0   6.0   \n",
      "3            000103501  4  PE6     1.0  35.0   4.0     1.0  1422.0   6.0   \n",
      "4            000103501  5  PE6     1.0  35.0   5.0     1.0  1422.0   5.0   \n",
      "...                   ...  ...     ...   ...   ...     ...     ...   ...   \n",
      "39740        325405401  4  PE6  3254.0  54.0   4.0  3254.0  1426.0   7.0   \n",
      "39741        325405701  2  PE6  3254.0  57.0   2.0  3254.0  1426.0   9.0   \n",
      "39742        325405801  3  PE6  3254.0  58.0   3.0  3254.0  1426.0  11.0   \n",
      "39743        325405901  2  PE6  3254.0  59.0   2.0  3254.0  1426.0  10.0   \n",
      "39744        325406101  2  PE6  3254.0  61.0   2.0  3254.0  1426.0   9.0   \n",
      "\n",
      "         V010    V011  ...    V508    V509  V510  V511  V512  V513  V525  \\\n",
      "0      1975.0   909.0  ...  1995.0  1141.0   6.0  19.0  23.0   5.0  18.0   \n",
      "1      2002.0  1227.0  ...     NaN     NaN   NaN   NaN   NaN   0.0   0.0   \n",
      "2      1997.0  1171.0  ...  2016.0  1399.0   1.0  19.0   1.0   1.0  17.0   \n",
      "3      2001.0  1219.0  ...     NaN     NaN   NaN   NaN   NaN   0.0  16.0   \n",
      "4      2006.0  1278.0  ...     NaN     NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "...       ...     ...  ...     ...     ...   ...   ...   ...   ...   ...   \n",
      "39740  2004.0  1255.0  ...     NaN     NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "39741  1977.0   933.0  ...  1996.0  1161.0   1.0  19.0  22.0   5.0  16.0   \n",
      "39742  2004.0  1260.0  ...     NaN     NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "39743  2000.0  1210.0  ...  2012.0  1355.0   1.0  12.0   5.0   2.0  12.0   \n",
      "39744  1988.0  1066.0  ...  2003.0  1243.0   1.0  14.0  15.0   4.0  14.0   \n",
      "\n",
      "       V613  V714  V715  \n",
      "0       2.0   1.0   6.0  \n",
      "1       2.0   0.0   NaN  \n",
      "2       2.0   0.0  11.0  \n",
      "3       2.0   0.0   NaN  \n",
      "4       NaN   NaN   NaN  \n",
      "...     ...   ...   ...  \n",
      "39740   NaN   NaN   NaN  \n",
      "39741   3.0   0.0   8.0  \n",
      "39742   NaN   NaN   NaN  \n",
      "39743   2.0   0.0  10.0  \n",
      "39744   4.0   0.0   7.0  \n",
      "\n",
      "[39745 rows x 63 columns], 'var_labels': {'year': 'Year of the survey'}, 'value_labels': {}}, 'year_2017': {'data':                    CASEID V000    V001   V002  V003    V004    V007    V008  \\\n",
      "0            000102501  2  PE6     1.0   25.0   2.0     1.0  2017.0  1409.0   \n",
      "1            000103201  2  PE6     1.0   32.0   2.0     1.0  2017.0  1409.0   \n",
      "2            000103601  2  PE6     1.0   36.0   2.0     1.0  2017.0  1409.0   \n",
      "3            000104501  2  PE6     1.0   45.0   2.0     1.0  2017.0  1409.0   \n",
      "4            000105201  2  PE6     1.0   52.0   2.0     1.0  2017.0  1409.0   \n",
      "...                   ...  ...     ...    ...   ...     ...     ...     ...   \n",
      "33997        317510001  2  PE6  3175.0  100.0   2.0  3175.0  2017.0  1413.0   \n",
      "33998        317510101  2  PE6  3175.0  101.0   2.0  3175.0  2017.0  1413.0   \n",
      "33999        317510301  2  PE6  3175.0  103.0   2.0  3175.0  2017.0  1413.0   \n",
      "34000        317510601  2  PE6  3175.0  106.0   2.0  3175.0  2017.0  1413.0   \n",
      "34001        317510701  2  PE6  3175.0  107.0   2.0  3175.0  2017.0  1413.0   \n",
      "\n",
      "       V009    V010  ...    V508    V509  V510  V511  V512  V513  V525  V613  \\\n",
      "0       3.0  1978.0  ...  1994.0  1136.0   5.0  16.0  22.0   5.0  16.0   2.0   \n",
      "1       2.0  1995.0  ...  2011.0  1344.0   1.0  16.0   5.0   2.0  16.0   2.0   \n",
      "2       5.0  1998.0  ...  2013.0  1368.0   1.0  15.0   3.0   1.0  15.0   0.0   \n",
      "3       4.0  1984.0  ...  2000.0  1211.0   1.0  16.0  16.0   4.0  14.0  96.0   \n",
      "4       4.0  1984.0  ...  2015.0  1386.0   1.0  31.0   1.0   1.0  17.0   2.0   \n",
      "...     ...     ...  ...     ...     ...   ...   ...   ...   ...   ...   ...   \n",
      "33997  11.0  1982.0  ...  1999.0  1196.0   1.0  16.0  18.0   4.0  14.0   2.0   \n",
      "33998   5.0  1975.0  ...  2005.0  1267.0   1.0  30.0  12.0   3.0  27.0  10.0   \n",
      "33999   5.0  1986.0  ...  2000.0  1205.0   1.0  14.0  17.0   4.0  13.0   3.0   \n",
      "34000   5.0  1992.0  ...  2008.0  1298.0   1.0  15.0   9.0   2.0  15.0  96.0   \n",
      "34001   2.0  1988.0  ...  2010.0  1321.0   1.0  21.0   7.0   2.0  21.0   5.0   \n",
      "\n",
      "       V714  V715  \n",
      "0       1.0   6.0  \n",
      "1       0.0  11.0  \n",
      "2       1.0  11.0  \n",
      "3       0.0   6.0  \n",
      "4       0.0  98.0  \n",
      "...     ...   ...  \n",
      "33997   1.0   3.0  \n",
      "33998   1.0   3.0  \n",
      "33999   1.0   1.0  \n",
      "34000   0.0   3.0  \n",
      "34001   1.0   5.0  \n",
      "\n",
      "[34002 rows x 64 columns], 'var_labels': {'year': 'Year of the survey'}, 'value_labels': {}}, 'year_2016': {'data':                    CASEID V000    V001  V002  V003    V004    V007    V008  \\\n",
      "0            000100301  2  PE6     1.0   3.0   2.0     1.0  2016.0  1398.0   \n",
      "1            000102801  1  PE6     1.0  28.0   1.0     1.0  2016.0  1398.0   \n",
      "2            000102801  2  PE6     1.0  28.0   2.0     1.0  2016.0  1398.0   \n",
      "3            000106101  2  PE6     1.0  61.0   2.0     1.0  2016.0  1398.0   \n",
      "4            000106801  2  PE6     1.0  68.0   2.0     1.0  2016.0  1398.0   \n",
      "...                   ...  ...     ...   ...   ...     ...     ...     ...   \n",
      "34126        317500701  2  PE6  3175.0   7.0   2.0  3175.0  2016.0  1401.0   \n",
      "34127        317501001  2  PE6  3175.0  10.0   2.0  3175.0  2016.0  1401.0   \n",
      "34128        317501101  2  PE6  3175.0  11.0   2.0  3175.0  2016.0  1401.0   \n",
      "34129        317501601  2  PE6  3175.0  16.0   2.0  3175.0  2016.0  1401.0   \n",
      "34130        317501801  2  PE6  3175.0  18.0   2.0  3175.0  2016.0  1401.0   \n",
      "\n",
      "       V009    V010  ...    V508    V509  V510  V511  V512  V513  V525  V613  \\\n",
      "0      11.0  1982.0  ...  2012.0  1352.0   1.0  29.0   3.0   1.0  19.0   2.0   \n",
      "1      10.0  1979.0  ...  1993.0  1123.0   5.0  13.0  22.0   5.0  13.0   5.0   \n",
      "2       6.0  1998.0  ...     NaN     NaN   NaN   NaN   NaN   0.0  16.0   1.0   \n",
      "3       4.0  1990.0  ...  2013.0  1366.0   1.0  23.0   2.0   1.0  20.0   4.0   \n",
      "4       6.0  1973.0  ...  1992.0  1105.0   1.0  18.0  24.0   5.0  18.0   2.0   \n",
      "...     ...     ...  ...     ...     ...   ...   ...   ...   ...   ...   ...   \n",
      "34126   4.0  1995.0  ...  2013.0  1365.0   1.0  18.0   3.0   1.0  13.0   2.0   \n",
      "34127   3.0  1978.0  ...  1990.0  1083.0   1.0  12.0  26.0   6.0  12.0   1.0   \n",
      "34128   2.0  2001.0  ...  2015.0  1389.0   1.0  14.0   1.0   1.0  14.0   2.0   \n",
      "34129   3.0  1987.0  ...  2004.0  1249.0   1.0  16.0  12.0   3.0  15.0   3.0   \n",
      "34130   5.0  1971.0  ...  1989.0  1078.0   1.0  18.0  26.0   6.0  13.0   2.0   \n",
      "\n",
      "       V714  V715  \n",
      "0       0.0   9.0  \n",
      "1       1.0   6.0  \n",
      "2       1.0   NaN  \n",
      "3       0.0  16.0  \n",
      "4       1.0   8.0  \n",
      "...     ...   ...  \n",
      "34126   0.0   9.0  \n",
      "34127   1.0   5.0  \n",
      "34128   1.0   6.0  \n",
      "34129   0.0   9.0  \n",
      "34130   1.0  11.0  \n",
      "\n",
      "[34131 rows x 64 columns], 'var_labels': {'year': 'Year of the survey'}, 'value_labels': {}}, 'year_2015': {'data':                    CASEID V000    V001  V002  V003    V004    V007    V008  \\\n",
      "0            000102701  1  PE6     1.0  27.0   1.0     1.0  2015.0  1386.0   \n",
      "1            000104301  1  PE6     1.0  43.0   1.0     1.0  2015.0  1386.0   \n",
      "2            000104801  2  PE6     1.0  48.0   2.0     1.0  2015.0  1386.0   \n",
      "3            000104801  3  PE6     1.0  48.0   3.0     1.0  2015.0  1386.0   \n",
      "4            000105001  3  PE6     1.0  50.0   3.0     1.0  2015.0  1386.0   \n",
      "...                   ...  ...     ...   ...   ...     ...     ...     ...   \n",
      "36650        317503501  2  PE6  3175.0  35.0   2.0  3175.0  2015.0  1389.0   \n",
      "36651        317503701  2  PE6  3175.0  37.0   2.0  3175.0  2015.0  1389.0   \n",
      "36652        317507601  1  PE6  3175.0  76.0   1.0  3175.0  2015.0  1389.0   \n",
      "36653        317507801  2  PE6  3175.0  78.0   2.0  3175.0  2015.0  1389.0   \n",
      "36654        317508001  2  PE6  3175.0  80.0   2.0  3175.0  2015.0  1389.0   \n",
      "\n",
      "       V009    V010  ...    V508    V509  V510  V511  V512  V513  V525  V613  \\\n",
      "0       7.0  1985.0  ...  2001.0  1220.0   1.0  16.0  13.0   3.0  15.0   4.0   \n",
      "1       4.0  1974.0  ...  2007.0  1290.0   1.0  33.0   8.0   2.0  26.0   2.0   \n",
      "2       1.0  1980.0  ...  1998.0  1177.0   1.0  18.0  17.0   4.0  18.0   1.0   \n",
      "3      11.0  1999.0  ...     NaN     NaN   NaN   NaN   NaN   0.0   0.0   0.0   \n",
      "4       8.0  1993.0  ...     NaN     NaN   NaN   NaN   NaN   0.0  21.0   2.0   \n",
      "...     ...     ...  ...     ...     ...   ...   ...   ...   ...   ...   ...   \n",
      "36650   1.0  1989.0  ...  2008.0  1303.0   1.0  19.0   7.0   2.0  15.0   1.0   \n",
      "36651  12.0  1970.0  ...  1991.0  1098.0   1.0  20.0  24.0   5.0  14.0   3.0   \n",
      "36652  10.0  1972.0  ...  1993.0  1118.0   1.0  20.0  22.0   5.0  12.0   3.0   \n",
      "36653   4.0  2000.0  ...     NaN     NaN   NaN   NaN   NaN   0.0   0.0   2.0   \n",
      "36654   7.0  1984.0  ...  2004.0  1250.0   1.0  19.0  11.0   3.0  18.0   2.0   \n",
      "\n",
      "       V714  V715  \n",
      "0       1.0   3.0  \n",
      "1       0.0   9.0  \n",
      "2       1.0  11.0  \n",
      "3       1.0   NaN  \n",
      "4       0.0   NaN  \n",
      "...     ...   ...  \n",
      "36650   1.0  11.0  \n",
      "36651   1.0   0.0  \n",
      "36652   1.0   3.0  \n",
      "36653   0.0   NaN  \n",
      "36654   0.0  11.0  \n",
      "\n",
      "[36655 rows x 64 columns], 'var_labels': {'year': 'Year of the survey'}, 'value_labels': {}}}\n"
     ]
    }
   ],
   "source": [
    "############ VERSIÓN LARGA, VOLVIENDO A DEFINIR LAS FUNCIONES YA USADAS EN LOS CÓDIGOS ANTERIORES\n",
    "############ ESTO NO ASUME QUE EL CÓDIGO DE LOS EJERCICIOS PREVIOS SEAN IGUALES\n",
    "\n",
    "# Let's define the years we are interested in and the base path for the data\n",
    "years = [2019, 2018, 2017, 2016, 2015]\n",
    "base_path = \"../../_data/endes/\"\n",
    "\n",
    "# Initialize an empty dictionary to store all data\n",
    "all_data = {}\n",
    "\n",
    "# OK, now we loop through each year to import data, select columns, update labels, and merge dataframes\n",
    "for year in years:\n",
    "    # First, we create a string for the dictionary key that includes the year\n",
    "    year_str = f\"year_{year}\"\n",
    "    \n",
    "    # Then, we build the path to the directory for the current year\n",
    "    path = f\"{base_path}{year}/\"\n",
    "    \n",
    "    # Now, we define the file paths for each dataset within the current year's directory\n",
    "    file1 = f\"{path}REC0111.sav\"\n",
    "    file2 = f\"{path}RE223132.sav\"\n",
    "    file3 = f\"{path}RE516171.sav\"\n",
    "    \n",
    "    # Next, we import the data and labels for each file using pyreadstat\n",
    "    rec1, meta1 = pyreadstat.read_sav(file1)\n",
    "    rec2, meta2 = pyreadstat.read_sav(file2)\n",
    "    rec3, meta3 = pyreadstat.read_sav(file3)\n",
    "    \n",
    "    # Just as we did in a previous exercise, we define the list of columns we are interested in\n",
    "    cols_rec1 = [\"CASEID\", \"V000\", \"V001\", \"V002\", \"V003\", \"V004\", \"V007\", \"V008\", \"V009\", \"V010\", \"V011\", \"V012\", \"V024\", \"V102\", \"V120\", \"V121\", \"V122\", \"V123\", \"V124\", \"V125\", \"V127\", \"V133\"]\n",
    "    cols_rec2 = [\"CASEID\", \"V201\", \"V218\", \"V301\", \"V302\", \"V323\", \"V323A\", \"V325A\", \"V326\", \"V327\", \"V337\", \"V359\", \"V360\", \"V361\", \"V362\", \"V363\", \"V364\", \"V367\", \"V372\", \"V372A\", \"V375A\", \"V376\", \"V376A\", \"V379\", \"V380\"]\n",
    "    cols_rec3 = [\"CASEID\", \"V501\", \"V502\", \"V503\", \"V504\", \"V505\", \"V506\", \"V507\", \"V508\", \"V509\", \"V510\", \"V511\", \"V512\", \"V513\", \"V525\", \"V613\", \"V714\", \"V715\"]\n",
    "\n",
    "    # Here we apply the function to check if the columns of our interest aren't missing and select them if they exist\n",
    "    def check_and_select_cols(df, cols, dataset_name):\n",
    "        selected_cols = [col for col in cols if col in df.columns]\n",
    "        missing_cols = [col for col in cols if col not in df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"Missing columns for {dataset_name} in year {year}: {missing_cols}\")\n",
    "        \n",
    "        return df.loc[:, selected_cols]\n",
    "    \n",
    "    # Applying the selection and check to each dataset\n",
    "    rec1_1 = check_and_select_cols(rec1, cols_rec1, \"rec1\")\n",
    "    rec2_1 = check_and_select_cols(rec2, cols_rec2, \"rec2\")\n",
    "    rec3_1 = check_and_select_cols(rec3, cols_rec3, \"rec3\")\n",
    "    \n",
    "    # For updating the variable labels for selected columns\n",
    "    def update_labels(old_labels, selected_cols):\n",
    "        return {key: old_labels[key] for key in selected_cols if key in old_labels}\n",
    "    \n",
    "    new_var_labels1 = update_labels(meta1.column_labels, cols_rec1)\n",
    "    new_var_labels2 = update_labels(meta2.column_labels, cols_rec2)\n",
    "    new_var_labels3 = update_labels(meta3.column_labels, cols_rec3)\n",
    "    \n",
    "    # For updating the value labels for selected columns\n",
    "    def update_value_labels(old_labels, selected_cols):\n",
    "        return {key: old_labels[key] for key in selected_cols if key in old_labels}\n",
    "    \n",
    "    new_value_labels1 = update_value_labels(meta1.value_labels, cols_rec1)\n",
    "    new_value_labels2 = update_value_labels(meta2.value_labels, cols_rec2)\n",
    "    new_value_labels3 = update_value_labels(meta3.value_labels, cols_rec3)\n",
    "    \n",
    "    # Create the 'year' column in rec1_1 and update the var_labels\n",
    "    rec1_1.loc[:, 'year'] = year\n",
    "    new_var_labels1.update({'year': \"Year of the survey\"})\n",
    "    \n",
    "    # Next, we merge the datasets using 'CASEID' and indicating 'outer' to capture all the wanted variables\n",
    "    endes_year = pd.merge(rec1_1, rec2_1, on='CASEID', how='outer')\n",
    "    endes_year = pd.merge(endes_year, rec3_1, on='CASEID', how='outer')\n",
    "    \n",
    "    # Then, we combine all the new_var_labels and new_value_labels into a single dictionary\n",
    "    var_labels = new_var_labels1.copy()\n",
    "    var_labels.update(new_var_labels2)\n",
    "    var_labels.update(new_var_labels3)\n",
    "    \n",
    "    value_labels = new_value_labels1.copy()\n",
    "    value_labels.update(new_value_labels2)\n",
    "    value_labels.update(new_value_labels3)\n",
    "    \n",
    "    # Here we add the new attributes to the corresponding endes_f\"{year}\" df\n",
    "    endes_year.attrs['var_labels'] = var_labels\n",
    "    endes_year.attrs['value_labels'] = value_labels\n",
    "    \n",
    "    # Finally, we store the processed data for the current year in the nested dictionary\n",
    "    all_data[year_str] = {\n",
    "        'data': endes_year,\n",
    "        'var_labels': var_labels,\n",
    "        'value_labels': value_labels\n",
    "    }\n",
    "\n",
    "# The 'all_data' dictionary now should contain the processed data for all the specified years\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns for rec1 in year 2018: ['V007']\n",
      "{'year_2019': {'data':                    CASEID V000    V001  V002  V003    V004    V007    V008  \\\n",
      "0            000100201  2  PE6     1.0   2.0   2.0     1.0  2019.0  1434.0   \n",
      "1            000100201  3  PE6     1.0   2.0   3.0     1.0  2019.0  1434.0   \n",
      "2            000102801  2  PE6     1.0  28.0   2.0     1.0  2019.0  1434.0   \n",
      "3            000102801  6  PE6     1.0  28.0   6.0     1.0  2019.0  1434.0   \n",
      "4            000104801  2  PE6     1.0  48.0   2.0     1.0  2019.0  1434.0   \n",
      "...                   ...  ...     ...   ...   ...     ...     ...     ...   \n",
      "38330        325406201  2  PE6  3254.0  62.0   2.0  3254.0  2019.0  1440.0   \n",
      "38331        325406301  2  PE6  3254.0  63.0   2.0  3254.0  2019.0  1440.0   \n",
      "38332        325407001  2  PE6  3254.0  70.0   2.0  3254.0  2019.0  1440.0   \n",
      "38333        325407201  2  PE6  3254.0  72.0   2.0  3254.0  2019.0  1440.0   \n",
      "38334        325407401  2  PE6  3254.0  74.0   2.0  3254.0  2019.0  1440.0   \n",
      "\n",
      "       V009    V010  ...    V508    V509  V510  V511  V512  V513  V525  V613  \\\n",
      "0       4.0  1986.0  ...  2008.0  1297.0   1.0  21.0  11.0   3.0  17.0   2.0   \n",
      "1       1.0  2007.0  ...     NaN     NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "2       6.0  1983.0  ...  2011.0  1344.0   1.0  28.0   7.0   2.0  18.0   3.0   \n",
      "3       3.0  1970.0  ...  1984.0  1016.0   5.0  14.0  34.0   7.0  14.0   0.0   \n",
      "4       5.0  1991.0  ...  2009.0  1320.0   1.0  18.0   9.0   2.0  15.0   2.0   \n",
      "...     ...     ...  ...     ...     ...   ...   ...   ...   ...   ...   ...   \n",
      "38330  12.0  1971.0  ...  1985.0  1032.0   1.0  14.0  34.0   7.0  12.0   3.0   \n",
      "38331   6.0  1988.0  ...  2002.0  1236.0   1.0  14.0  17.0   4.0  12.0   3.0   \n",
      "38332   7.0  1973.0  ...  1986.0  1043.0   1.0  13.0  33.0   7.0  12.0   4.0   \n",
      "38333  12.0  1994.0  ...  2008.0  1301.0   1.0  13.0  11.0   3.0  13.0   2.0   \n",
      "38334  10.0  1996.0  ...  2013.0  1362.0   1.0  16.0   6.0   2.0  15.0   2.0   \n",
      "\n",
      "       V714  V715  \n",
      "0       1.0  11.0  \n",
      "1       NaN   NaN  \n",
      "2       1.0  14.0  \n",
      "3       0.0   6.0  \n",
      "4       0.0   6.0  \n",
      "...     ...   ...  \n",
      "38330   0.0   5.0  \n",
      "38331   0.0   7.0  \n",
      "38332   0.0   5.0  \n",
      "38333   0.0  11.0  \n",
      "38334   0.0  11.0  \n",
      "\n",
      "[38335 rows x 64 columns], 'var_labels': {'year': 'Year of the survey'}, 'value_labels': {}}, 'year_2018': {'data':                    CASEID V000    V001  V002  V003    V004    V008  V009  \\\n",
      "0            000100701  2  PE6     1.0   7.0   2.0     1.0  1422.0   8.0   \n",
      "1            000100701  3  PE6     1.0   7.0   3.0     1.0  1422.0   2.0   \n",
      "2            000101401  2  PE6     1.0  14.0   2.0     1.0  1422.0   6.0   \n",
      "3            000103501  4  PE6     1.0  35.0   4.0     1.0  1422.0   6.0   \n",
      "4            000103501  5  PE6     1.0  35.0   5.0     1.0  1422.0   5.0   \n",
      "...                   ...  ...     ...   ...   ...     ...     ...   ...   \n",
      "39740        325405401  4  PE6  3254.0  54.0   4.0  3254.0  1426.0   7.0   \n",
      "39741        325405701  2  PE6  3254.0  57.0   2.0  3254.0  1426.0   9.0   \n",
      "39742        325405801  3  PE6  3254.0  58.0   3.0  3254.0  1426.0  11.0   \n",
      "39743        325405901  2  PE6  3254.0  59.0   2.0  3254.0  1426.0  10.0   \n",
      "39744        325406101  2  PE6  3254.0  61.0   2.0  3254.0  1426.0   9.0   \n",
      "\n",
      "         V010    V011  ...    V508    V509  V510  V511  V512  V513  V525  \\\n",
      "0      1975.0   909.0  ...  1995.0  1141.0   6.0  19.0  23.0   5.0  18.0   \n",
      "1      2002.0  1227.0  ...     NaN     NaN   NaN   NaN   NaN   0.0   0.0   \n",
      "2      1997.0  1171.0  ...  2016.0  1399.0   1.0  19.0   1.0   1.0  17.0   \n",
      "3      2001.0  1219.0  ...     NaN     NaN   NaN   NaN   NaN   0.0  16.0   \n",
      "4      2006.0  1278.0  ...     NaN     NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "...       ...     ...  ...     ...     ...   ...   ...   ...   ...   ...   \n",
      "39740  2004.0  1255.0  ...     NaN     NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "39741  1977.0   933.0  ...  1996.0  1161.0   1.0  19.0  22.0   5.0  16.0   \n",
      "39742  2004.0  1260.0  ...     NaN     NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "39743  2000.0  1210.0  ...  2012.0  1355.0   1.0  12.0   5.0   2.0  12.0   \n",
      "39744  1988.0  1066.0  ...  2003.0  1243.0   1.0  14.0  15.0   4.0  14.0   \n",
      "\n",
      "       V613  V714  V715  \n",
      "0       2.0   1.0   6.0  \n",
      "1       2.0   0.0   NaN  \n",
      "2       2.0   0.0  11.0  \n",
      "3       2.0   0.0   NaN  \n",
      "4       NaN   NaN   NaN  \n",
      "...     ...   ...   ...  \n",
      "39740   NaN   NaN   NaN  \n",
      "39741   3.0   0.0   8.0  \n",
      "39742   NaN   NaN   NaN  \n",
      "39743   2.0   0.0  10.0  \n",
      "39744   4.0   0.0   7.0  \n",
      "\n",
      "[39745 rows x 63 columns], 'var_labels': {'year': 'Year of the survey'}, 'value_labels': {}}, 'year_2017': {'data':                    CASEID V000    V001   V002  V003    V004    V007    V008  \\\n",
      "0            000102501  2  PE6     1.0   25.0   2.0     1.0  2017.0  1409.0   \n",
      "1            000103201  2  PE6     1.0   32.0   2.0     1.0  2017.0  1409.0   \n",
      "2            000103601  2  PE6     1.0   36.0   2.0     1.0  2017.0  1409.0   \n",
      "3            000104501  2  PE6     1.0   45.0   2.0     1.0  2017.0  1409.0   \n",
      "4            000105201  2  PE6     1.0   52.0   2.0     1.0  2017.0  1409.0   \n",
      "...                   ...  ...     ...    ...   ...     ...     ...     ...   \n",
      "33997        317510001  2  PE6  3175.0  100.0   2.0  3175.0  2017.0  1413.0   \n",
      "33998        317510101  2  PE6  3175.0  101.0   2.0  3175.0  2017.0  1413.0   \n",
      "33999        317510301  2  PE6  3175.0  103.0   2.0  3175.0  2017.0  1413.0   \n",
      "34000        317510601  2  PE6  3175.0  106.0   2.0  3175.0  2017.0  1413.0   \n",
      "34001        317510701  2  PE6  3175.0  107.0   2.0  3175.0  2017.0  1413.0   \n",
      "\n",
      "       V009    V010  ...    V508    V509  V510  V511  V512  V513  V525  V613  \\\n",
      "0       3.0  1978.0  ...  1994.0  1136.0   5.0  16.0  22.0   5.0  16.0   2.0   \n",
      "1       2.0  1995.0  ...  2011.0  1344.0   1.0  16.0   5.0   2.0  16.0   2.0   \n",
      "2       5.0  1998.0  ...  2013.0  1368.0   1.0  15.0   3.0   1.0  15.0   0.0   \n",
      "3       4.0  1984.0  ...  2000.0  1211.0   1.0  16.0  16.0   4.0  14.0  96.0   \n",
      "4       4.0  1984.0  ...  2015.0  1386.0   1.0  31.0   1.0   1.0  17.0   2.0   \n",
      "...     ...     ...  ...     ...     ...   ...   ...   ...   ...   ...   ...   \n",
      "33997  11.0  1982.0  ...  1999.0  1196.0   1.0  16.0  18.0   4.0  14.0   2.0   \n",
      "33998   5.0  1975.0  ...  2005.0  1267.0   1.0  30.0  12.0   3.0  27.0  10.0   \n",
      "33999   5.0  1986.0  ...  2000.0  1205.0   1.0  14.0  17.0   4.0  13.0   3.0   \n",
      "34000   5.0  1992.0  ...  2008.0  1298.0   1.0  15.0   9.0   2.0  15.0  96.0   \n",
      "34001   2.0  1988.0  ...  2010.0  1321.0   1.0  21.0   7.0   2.0  21.0   5.0   \n",
      "\n",
      "       V714  V715  \n",
      "0       1.0   6.0  \n",
      "1       0.0  11.0  \n",
      "2       1.0  11.0  \n",
      "3       0.0   6.0  \n",
      "4       0.0  98.0  \n",
      "...     ...   ...  \n",
      "33997   1.0   3.0  \n",
      "33998   1.0   3.0  \n",
      "33999   1.0   1.0  \n",
      "34000   0.0   3.0  \n",
      "34001   1.0   5.0  \n",
      "\n",
      "[34002 rows x 64 columns], 'var_labels': {'year': 'Year of the survey'}, 'value_labels': {}}, 'year_2016': {'data':                    CASEID V000    V001  V002  V003    V004    V007    V008  \\\n",
      "0            000100301  2  PE6     1.0   3.0   2.0     1.0  2016.0  1398.0   \n",
      "1            000102801  1  PE6     1.0  28.0   1.0     1.0  2016.0  1398.0   \n",
      "2            000102801  2  PE6     1.0  28.0   2.0     1.0  2016.0  1398.0   \n",
      "3            000106101  2  PE6     1.0  61.0   2.0     1.0  2016.0  1398.0   \n",
      "4            000106801  2  PE6     1.0  68.0   2.0     1.0  2016.0  1398.0   \n",
      "...                   ...  ...     ...   ...   ...     ...     ...     ...   \n",
      "34126        317500701  2  PE6  3175.0   7.0   2.0  3175.0  2016.0  1401.0   \n",
      "34127        317501001  2  PE6  3175.0  10.0   2.0  3175.0  2016.0  1401.0   \n",
      "34128        317501101  2  PE6  3175.0  11.0   2.0  3175.0  2016.0  1401.0   \n",
      "34129        317501601  2  PE6  3175.0  16.0   2.0  3175.0  2016.0  1401.0   \n",
      "34130        317501801  2  PE6  3175.0  18.0   2.0  3175.0  2016.0  1401.0   \n",
      "\n",
      "       V009    V010  ...    V508    V509  V510  V511  V512  V513  V525  V613  \\\n",
      "0      11.0  1982.0  ...  2012.0  1352.0   1.0  29.0   3.0   1.0  19.0   2.0   \n",
      "1      10.0  1979.0  ...  1993.0  1123.0   5.0  13.0  22.0   5.0  13.0   5.0   \n",
      "2       6.0  1998.0  ...     NaN     NaN   NaN   NaN   NaN   0.0  16.0   1.0   \n",
      "3       4.0  1990.0  ...  2013.0  1366.0   1.0  23.0   2.0   1.0  20.0   4.0   \n",
      "4       6.0  1973.0  ...  1992.0  1105.0   1.0  18.0  24.0   5.0  18.0   2.0   \n",
      "...     ...     ...  ...     ...     ...   ...   ...   ...   ...   ...   ...   \n",
      "34126   4.0  1995.0  ...  2013.0  1365.0   1.0  18.0   3.0   1.0  13.0   2.0   \n",
      "34127   3.0  1978.0  ...  1990.0  1083.0   1.0  12.0  26.0   6.0  12.0   1.0   \n",
      "34128   2.0  2001.0  ...  2015.0  1389.0   1.0  14.0   1.0   1.0  14.0   2.0   \n",
      "34129   3.0  1987.0  ...  2004.0  1249.0   1.0  16.0  12.0   3.0  15.0   3.0   \n",
      "34130   5.0  1971.0  ...  1989.0  1078.0   1.0  18.0  26.0   6.0  13.0   2.0   \n",
      "\n",
      "       V714  V715  \n",
      "0       0.0   9.0  \n",
      "1       1.0   6.0  \n",
      "2       1.0   NaN  \n",
      "3       0.0  16.0  \n",
      "4       1.0   8.0  \n",
      "...     ...   ...  \n",
      "34126   0.0   9.0  \n",
      "34127   1.0   5.0  \n",
      "34128   1.0   6.0  \n",
      "34129   0.0   9.0  \n",
      "34130   1.0  11.0  \n",
      "\n",
      "[34131 rows x 64 columns], 'var_labels': {'year': 'Year of the survey'}, 'value_labels': {}}, 'year_2015': {'data':                    CASEID V000    V001  V002  V003    V004    V007    V008  \\\n",
      "0            000102701  1  PE6     1.0  27.0   1.0     1.0  2015.0  1386.0   \n",
      "1            000104301  1  PE6     1.0  43.0   1.0     1.0  2015.0  1386.0   \n",
      "2            000104801  2  PE6     1.0  48.0   2.0     1.0  2015.0  1386.0   \n",
      "3            000104801  3  PE6     1.0  48.0   3.0     1.0  2015.0  1386.0   \n",
      "4            000105001  3  PE6     1.0  50.0   3.0     1.0  2015.0  1386.0   \n",
      "...                   ...  ...     ...   ...   ...     ...     ...     ...   \n",
      "36650        317503501  2  PE6  3175.0  35.0   2.0  3175.0  2015.0  1389.0   \n",
      "36651        317503701  2  PE6  3175.0  37.0   2.0  3175.0  2015.0  1389.0   \n",
      "36652        317507601  1  PE6  3175.0  76.0   1.0  3175.0  2015.0  1389.0   \n",
      "36653        317507801  2  PE6  3175.0  78.0   2.0  3175.0  2015.0  1389.0   \n",
      "36654        317508001  2  PE6  3175.0  80.0   2.0  3175.0  2015.0  1389.0   \n",
      "\n",
      "       V009    V010  ...    V508    V509  V510  V511  V512  V513  V525  V613  \\\n",
      "0       7.0  1985.0  ...  2001.0  1220.0   1.0  16.0  13.0   3.0  15.0   4.0   \n",
      "1       4.0  1974.0  ...  2007.0  1290.0   1.0  33.0   8.0   2.0  26.0   2.0   \n",
      "2       1.0  1980.0  ...  1998.0  1177.0   1.0  18.0  17.0   4.0  18.0   1.0   \n",
      "3      11.0  1999.0  ...     NaN     NaN   NaN   NaN   NaN   0.0   0.0   0.0   \n",
      "4       8.0  1993.0  ...     NaN     NaN   NaN   NaN   NaN   0.0  21.0   2.0   \n",
      "...     ...     ...  ...     ...     ...   ...   ...   ...   ...   ...   ...   \n",
      "36650   1.0  1989.0  ...  2008.0  1303.0   1.0  19.0   7.0   2.0  15.0   1.0   \n",
      "36651  12.0  1970.0  ...  1991.0  1098.0   1.0  20.0  24.0   5.0  14.0   3.0   \n",
      "36652  10.0  1972.0  ...  1993.0  1118.0   1.0  20.0  22.0   5.0  12.0   3.0   \n",
      "36653   4.0  2000.0  ...     NaN     NaN   NaN   NaN   NaN   0.0   0.0   2.0   \n",
      "36654   7.0  1984.0  ...  2004.0  1250.0   1.0  19.0  11.0   3.0  18.0   2.0   \n",
      "\n",
      "       V714  V715  \n",
      "0       1.0   3.0  \n",
      "1       0.0   9.0  \n",
      "2       1.0  11.0  \n",
      "3       1.0   NaN  \n",
      "4       0.0   NaN  \n",
      "...     ...   ...  \n",
      "36650   1.0  11.0  \n",
      "36651   1.0   0.0  \n",
      "36652   1.0   3.0  \n",
      "36653   0.0   NaN  \n",
      "36654   0.0  11.0  \n",
      "\n",
      "[36655 rows x 64 columns], 'var_labels': {'year': 'Year of the survey'}, 'value_labels': {}}}\n"
     ]
    }
   ],
   "source": [
    "#######################  [ESTA ES LA VERSIÓN CORTA, SIMPLEMENTE LLAMANDO A LAS FUNCIONES YA DEFINIDAS ANTES]\n",
    "#######################  [ESTA VERSIÓN ASUME QUE EL CÓDIGO ES EL MISMO, POR ESO SOLO LLAMA A LAS FUNCIONES]\n",
    "\n",
    "\n",
    "# We begin by defining the years we are interested in and the base path for the data\n",
    "years = [2019, 2018, 2017, 2016, 2015]\n",
    "base_path = \"../../_data/endes/\"\n",
    "\n",
    "# Below, we define an empty dictionary to store all data\n",
    "all_data = {}\n",
    "\n",
    "# OK, now we loop through each year to import data, select columns, update labels, and merge df\n",
    "for year in years:\n",
    "    # First, we create a string for the dictionary key that includes the year\n",
    "    year_str = f\"year_{year}\"\n",
    "    \n",
    "    # For building the path to the directory for the current year, we use\n",
    "    path = f\"{base_path}{year}/\"\n",
    "    \n",
    "    # We define the file paths for each dataset within the current year's directory\n",
    "    file1 = f\"{path}REC0111.sav\"\n",
    "    file2 = f\"{path}RE223132.sav\"\n",
    "    file3 = f\"{path}RE516171.sav\"\n",
    "    \n",
    "    # Next, we import the data and labels for each file using pyreadstat\n",
    "    rec1, meta1 = pyreadstat.read_sav(file1)\n",
    "    rec2, meta2 = pyreadstat.read_sav(file2)\n",
    "    rec3, meta3 = pyreadstat.read_sav(file3)\n",
    "    \n",
    "    # Here we apply the function we defined in exercise 2 to check if our columns of interest are not missing and select them\n",
    "    rec1_1 = check_and_select_cols(rec1, cols_rec1, \"rec1\")\n",
    "    rec2_1 = check_and_select_cols(rec2, cols_rec2, \"rec2\")\n",
    "    rec3_1 = check_and_select_cols(rec3, cols_rec3, \"rec3\")\n",
    "    \n",
    "    # This is to update the variable labels, using the function from exercise 2\n",
    "    new_var_labels1 = update_labels(meta1.column_labels, cols_rec1)\n",
    "    new_var_labels2 = update_labels(meta2.column_labels, cols_rec2)\n",
    "    new_var_labels3 = update_labels(meta3.column_labels, cols_rec3)\n",
    "    \n",
    "    # Another function from ex.2, this one, for updating the value labels\n",
    "    new_value_labels1 = update_value_labels(meta1.value_labels, cols_rec1)\n",
    "    new_value_labels2 = update_value_labels(meta2.value_labels, cols_rec2)\n",
    "    new_value_labels3 = update_value_labels(meta3.value_labels, cols_rec3)\n",
    "    \n",
    "    # Let's create the 'year' column in rec1_1 and update the var_labels\n",
    "    rec1_1.loc[:, 'year'] = year\n",
    "    new_var_labels1.update({'year': \"Year of the survey\"})\n",
    "    \n",
    "    # Next, we merge the datasets using 'CASEID' and indicating 'outer' to capture all the wanted variables\n",
    "    endes_year = pd.merge(rec1_1, rec2_1, on='CASEID', how='outer')\n",
    "    endes_year = pd.merge(endes_year, rec3_1, on='CASEID', how='outer')\n",
    "    \n",
    "    # Then, we combine all the new_var_labels and new_value_labels into a single dictionary\n",
    "    var_labels = new_var_labels1.copy()\n",
    "    var_labels.update(new_var_labels2)\n",
    "    var_labels.update(new_var_labels3)\n",
    "    \n",
    "    value_labels = new_value_labels1.copy()\n",
    "    value_labels.update(new_value_labels2)\n",
    "    value_labels.update(new_value_labels3)\n",
    "    \n",
    "    # Here we add the new attributes to the corresponding endes_f\"{year}\" df\n",
    "    endes_year.attrs['var_labels'] = var_labels\n",
    "    endes_year.attrs['value_labels'] = value_labels\n",
    "    \n",
    "    # Finally, we store the processed data for the current year in the nested dictionary\n",
    "    all_data[year_str] = {\n",
    "        'data': endes_year,\n",
    "        'var_labels': var_labels,\n",
    "        'value_labels': value_labels\n",
    "    }\n",
    "\n",
    "# The 'all_data' dictionary now should contain the processed data for all the specified years\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use `all_data` to append all the data sets. Store all data sets in a list using `for loop`. Then, use `pd.concat` to append all the data sets. Also, you must reset the index to have a good-looking data. This new object should be named as `endes_data_2015_2019`. **Hint: Use [this code](https://stackoverflow.com/questions/32444138/concatenate-a-list-of-pandas-dataframes-together)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    CASEID V000    V001  V002  V003    V004    V007    V008  \\\n",
      "0             000100201  2  PE6     1.0   2.0   2.0     1.0  2019.0  1434.0   \n",
      "1             000100201  3  PE6     1.0   2.0   3.0     1.0  2019.0  1434.0   \n",
      "2             000102801  2  PE6     1.0  28.0   2.0     1.0  2019.0  1434.0   \n",
      "3             000102801  6  PE6     1.0  28.0   6.0     1.0  2019.0  1434.0   \n",
      "4             000104801  2  PE6     1.0  48.0   2.0     1.0  2019.0  1434.0   \n",
      "...                    ...  ...     ...   ...   ...     ...     ...     ...   \n",
      "182863        317503501  2  PE6  3175.0  35.0   2.0  3175.0  2015.0  1389.0   \n",
      "182864        317503701  2  PE6  3175.0  37.0   2.0  3175.0  2015.0  1389.0   \n",
      "182865        317507601  1  PE6  3175.0  76.0   1.0  3175.0  2015.0  1389.0   \n",
      "182866        317507801  2  PE6  3175.0  78.0   2.0  3175.0  2015.0  1389.0   \n",
      "182867        317508001  2  PE6  3175.0  80.0   2.0  3175.0  2015.0  1389.0   \n",
      "\n",
      "        V009    V010  ...    V508    V509  V510  V511  V512  V513  V525  V613  \\\n",
      "0        4.0  1986.0  ...  2008.0  1297.0   1.0  21.0  11.0   3.0  17.0   2.0   \n",
      "1        1.0  2007.0  ...     NaN     NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "2        6.0  1983.0  ...  2011.0  1344.0   1.0  28.0   7.0   2.0  18.0   3.0   \n",
      "3        3.0  1970.0  ...  1984.0  1016.0   5.0  14.0  34.0   7.0  14.0   0.0   \n",
      "4        5.0  1991.0  ...  2009.0  1320.0   1.0  18.0   9.0   2.0  15.0   2.0   \n",
      "...      ...     ...  ...     ...     ...   ...   ...   ...   ...   ...   ...   \n",
      "182863   1.0  1989.0  ...  2008.0  1303.0   1.0  19.0   7.0   2.0  15.0   1.0   \n",
      "182864  12.0  1970.0  ...  1991.0  1098.0   1.0  20.0  24.0   5.0  14.0   3.0   \n",
      "182865  10.0  1972.0  ...  1993.0  1118.0   1.0  20.0  22.0   5.0  12.0   3.0   \n",
      "182866   4.0  2000.0  ...     NaN     NaN   NaN   NaN   NaN   0.0   0.0   2.0   \n",
      "182867   7.0  1984.0  ...  2004.0  1250.0   1.0  19.0  11.0   3.0  18.0   2.0   \n",
      "\n",
      "        V714  V715  \n",
      "0        1.0  11.0  \n",
      "1        NaN   NaN  \n",
      "2        1.0  14.0  \n",
      "3        0.0   6.0  \n",
      "4        0.0   6.0  \n",
      "...      ...   ...  \n",
      "182863   1.0  11.0  \n",
      "182864   1.0   0.0  \n",
      "182865   1.0   3.0  \n",
      "182866   0.0   NaN  \n",
      "182867   0.0  11.0  \n",
      "\n",
      "[182868 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# We create an empty list to store all the df from all years\n",
    "data_frames = []\n",
    "\n",
    "# Now, we'll loop through each year's data in all_data\n",
    "for year_str in all_data.keys():\n",
    "\n",
    "    # Extract the DataFrame from the current year's dictionary, not the entire list\n",
    "    year_df = all_data[year_str]['data']\n",
    "    \n",
    "    # Extend the DataFrame into the data_frames list\n",
    "    # By using extend instead of append, we avoid nesting lists within the list (which is what the last does)\n",
    "    data_frames.append(year_df)\n",
    "\n",
    "# Lastly, we concatenate all df into a single dataframe row-wise (axis=0)\n",
    "  # Why row-wise? Because our df has the same columns but for different cases (persons)\n",
    "  # We also set 'ignore_index = True' to avoid having repeated indexes (0, 1, 2, 4, 0, 1, 2, 3...)\n",
    "endes_data_2015_2019 = pd.concat(data_frames, axis = 0, ignore_index = True)\n",
    "\n",
    "# We check the output. Remember, the number of columns should be 64, we explained it in the code of exercise 4\n",
    "print(endes_data_2015_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Store all the `var_labels` and `value_labels` in a dictionary named as `all_var_labels` and `all_value_labels`. The first keys should be the year for both dictionaries.Then, use them to generate new attributes for `endes_data_2015_2019`. These attributes should be named as `var_labels` and `value_labels`.  **Hint: Use [this link](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=4d6de78e00e7001f16bf6473c2eb7ce24fb611cd&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f616c6578616e6465727175697370652f4469706c6f6d61646f5f505543502f346436646537386530306537303031663136626636343733633265623763653234666236313163642f4c6563747572655f342f4c6563747572655f342e6970796e62&logged_in=true&nwo=alexanderquispe%2FDiplomado_PUCP&path=Lecture_4%2FLecture_4.ipynb&platform=windows&repository_id=427747212&repository_type=Repository&version=95#4.2.3.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Use `endes_data_2015_2019` data to generate a new object named `mean_key_vars` to find the mean of **total children ever born (V201)**, **Ideal number of children (V613)**, **Husbands education-single yrs (V715)**, and **Age at first marriage (V511)** by year and department **(V024)**. Name these columns as **mean_total_children, mean_ideal_children, mean_hb_yr_educ and mean_first_marriage**, respectively. **Hint: Use groupby and [this link](https://stackoverflow.com/questions/40901770/is-there-a-simple-way-to-change-a-column-of-yes-no-to-1-0-in-a-pandas-dataframe).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Merge `mean_key_vars` with `endes_data_2015_2019`. Name this object `final_result`. **Hint: Use merge.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
