{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group_1\n",
    "- MAX LENIN CHIPANI LIMA\n",
    "- ANDRES ALEXANDER VILLACORTA BARRERA\n",
    "- ESTEFANNY MIRIAN GIL MAMANI\n",
    "- VANESSA ALESSANDRA AZAÑEDO GAMARRAamarena\n",
    "- JOSE MARIA MIGUEL LOYOLA ROMERO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyreadstat\n",
      "  Downloading pyreadstat-1.2.7-cp312-cp312-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\josel\\anaconda3\\lib\\site-packages (from pyreadstat) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\josel\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\josel\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\josel\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\josel\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\josel\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadstat) (1.16.0)\n",
      "Downloading pyreadstat-1.2.7-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.9/2.4 MB 8.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.6/2.4 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 11.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pyreadstat\n",
      "Successfully installed pyreadstat-1.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import pyreadstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their variables and values labels from this path `\"../../_data/endes/2019\"`. The name of imported files should be named as `rec_1`, `rec_2` and `rec_3` for files `REC0111.sav`, `RE223132.sav` and `RE516171.sav` respectively. The name of the variable and value labels should be `var_labels1` and `value_labels1` for `rec1`, `var_labels2` and `value_labels2` for `rec2`, and `var_labels3` and `value_labels3` for `rec3`. **Hint: See the section 3.3.4 of [the lecture 3](https://github.com/alexanderquispe/Diplomado_PUCP/blob/main/Lecture_3/Lecture_3.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "\n",
    "## Define the file paths. \n",
    "file_path_1 = \"../../_data/endes/2019/REC0111.sav\"\n",
    "file_path_2 = \"../../_data/endes/2019/RE223132.sav\"\n",
    "file_path_3 = \"../../_data/endes/2019/RE516171.sav\"\n",
    "\n",
    "## Load the .sav files. Use pyreadstat.read_sav instead of pandas.read_spss as it allows to access the metadata; for instance, the variable and value labels.\n",
    "rec_1, meta1 = pyreadstat.read_sav(file_path_1)\n",
    "rec_2, meta2 = pyreadstat.read_sav(file_path_2)\n",
    "rec_3, meta3 = pyreadstat.read_sav(file_path_3)\n",
    "\n",
    "## Extract variable and value labels. \n",
    "var_labels1 = meta1.column_labels\n",
    "value_labels1 = meta1.variable_value_labels\n",
    "\n",
    "var_labels2 = meta2.column_labels\n",
    "value_labels2 = meta2.variable_value_labels\n",
    "\n",
    "var_labels3 = meta3.column_labels\n",
    "value_labels3 = meta3.variable_value_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select the following columns for each data set. Check if all the columns are in the dataset. Make a code that check the columns that are not included. Please, reporte them.\n",
    "\n",
    "|Data|Columns|\n",
    "|---|---|\n",
    "|rec1| CASEID, V000, V001, V002, V003, V004, V007, V008, V009, V010, V011, V012, V024, V102, V120, V121, V122, V123, V124, V125, V127, V133 |\n",
    "|rec2| CASEID, V201, V218, V301, V302, V323, V323A, V325A, V326, V327, V337, V359, V360, V361, V362, V363, V364, V367, V372, V372A, V375A, V376, V376A, V379, V380 |\n",
    "|rec3| CASEID, V501, V502, V503, V504, V505, V506, V507, V508, V509, V510, V511, V512, V513, V525, V613, V714, V715 |\n",
    "\n",
    "\n",
    "Additioanlly, you should update the variables and value labels objects. They must have information only for the selected columns. The new dataframes should be name as `rec1_1`, `rec2_1`, and `rec3_1`. The new varible labels objects should be named as `new_var_labels1`, `new_var_labels2`, and `new_var_labels3`. The new value labels objects should be named as `new_value_labels1`, `new_value_labels2`, and `new_value_labels3` **Hint: Use the `loc` and column names to filter, `for loop`,   and [this link](https://stackoverflow.com/questions/3420122/filter-dict-to-contain-only-certain-keys) to update the var and value dictionary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns are present in rec_1.\n",
      "All columns are present in rec_2.\n",
      "All columns are present in rec_3.\n"
     ]
    }
   ],
   "source": [
    "import pyreadstat\n",
    "\n",
    "# Define the file paths\n",
    "file_path_1 = \"../../_data/endes/2019/REC0111.sav\"\n",
    "file_path_2 = \"../../_data/endes/2019/RE223132.sav\"\n",
    "file_path_3 = \"../../_data/endes/2019/RE516171.sav\"\n",
    "\n",
    "# Load the .sav files along with their metadata\n",
    "rec_1, meta1 = pyreadstat.read_sav(file_path_1)\n",
    "rec_2, meta2 = pyreadstat.read_sav(file_path_2)\n",
    "rec_3, meta3 = pyreadstat.read_sav(file_path_3)\n",
    "\n",
    "# Define the columns to select for each dataset\n",
    "columns_1 = ['CASEID', 'V000', 'V001', 'V002', 'V003', 'V004', 'V007', 'V008', 'V009', \n",
    "             'V010', 'V011', 'V012', 'V024', 'V102', 'V120', 'V121', 'V122', 'V123', \n",
    "             'V124', 'V125', 'V127', 'V133']\n",
    "\n",
    "columns_2 = ['CASEID', 'V201', 'V218', 'V301', 'V302', 'V323', 'V323A', 'V325A', 'V326', \n",
    "             'V327', 'V337', 'V359', 'V360', 'V361', 'V362', 'V363', 'V364', 'V367', \n",
    "             'V372', 'V372A', 'V375A', 'V376', 'V376A', 'V379', 'V380']\n",
    "\n",
    "columns_3 = ['CASEID', 'V501', 'V502', 'V503', 'V504', 'V505', 'V506', 'V507', 'V508', \n",
    "             'V509', 'V510', 'V511', 'V512', 'V513', 'V525', 'V613', 'V714', 'V715']\n",
    "\n",
    "# Function to check and report missing columns, and filter the DataFrame using loc\n",
    "def check_and_filter_columns(dataset, metadata, column_list, dataset_name):\n",
    "    missing_columns = [col for col in column_list if col not in dataset.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"The following columns are missing from {dataset_name}: {missing_columns}\")\n",
    "    else:\n",
    "        print(f\"All columns are present in {dataset_name}.\")\n",
    "    \n",
    "    # Filter the DataFrame using .loc to include only the selected columns\n",
    "    selected_columns = [col for col in column_list if col in dataset.columns]\n",
    "    filtered_data = dataset.loc[:, selected_columns]\n",
    "\n",
    "    # Check if column_labels and variable_value_labels are dictionaries\n",
    "    if isinstance(metadata.column_labels, dict):\n",
    "        filtered_var_labels = {k: metadata.column_labels[k] for k in selected_columns}\n",
    "    else:\n",
    "        filtered_var_labels = {k: metadata.column_labels[metadata.column_names.index(k)] for k in selected_columns}\n",
    "\n",
    "    if isinstance(metadata.variable_value_labels, dict):\n",
    "        filtered_value_labels = {k: metadata.variable_value_labels[k] for k in selected_columns if k in metadata.variable_value_labels}\n",
    "    else:\n",
    "        filtered_value_labels = {k: metadata.variable_value_labels[metadata.column_names.index(k)] for k in selected_columns if k in metadata.variable_value_labels}\n",
    "    \n",
    "    return filtered_data, filtered_var_labels, filtered_value_labels\n",
    "\n",
    "# Apply the function to each dataset\n",
    "rec1_1, new_var_labels1, new_value_labels1 = check_and_filter_columns(rec_1, meta1, columns_1, 'rec_1')\n",
    "rec2_1, new_var_labels2, new_value_labels2 = check_and_filter_columns(rec_2, meta2, columns_2, 'rec_2')\n",
    "rec3_1, new_var_labels3, new_value_labels3 = check_and_filter_columns(rec_3, meta3, columns_3, 'rec_3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate a new column for `rec1_1` named as `year`. It should be equal to `2019`. Also, you must update this new variable for the `var_labels` dictionary. Generate a new key for `new_var_labels1` and the value for this key should be **\"Year of the survey\"** **Hint: Use `loc` and `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASEID</th>\n",
       "      <th>V000</th>\n",
       "      <th>V001</th>\n",
       "      <th>V002</th>\n",
       "      <th>V003</th>\n",
       "      <th>V004</th>\n",
       "      <th>V007</th>\n",
       "      <th>V008</th>\n",
       "      <th>V009</th>\n",
       "      <th>V010</th>\n",
       "      <th>...</th>\n",
       "      <th>V102</th>\n",
       "      <th>V120</th>\n",
       "      <th>V121</th>\n",
       "      <th>V122</th>\n",
       "      <th>V123</th>\n",
       "      <th>V124</th>\n",
       "      <th>V125</th>\n",
       "      <th>V127</th>\n",
       "      <th>V133</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000100201  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000100201  3</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000102801  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000102801  6</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000104801  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38330</th>\n",
       "      <td>325406201  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38331</th>\n",
       "      <td>325406301  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38332</th>\n",
       "      <td>325407001  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38333</th>\n",
       "      <td>325407201  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38334</th>\n",
       "      <td>325407401  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38335 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   CASEID V000    V001  V002  V003    V004    V007    V008  \\\n",
       "0            000100201  2  PE6     1.0   2.0   2.0     1.0  2019.0  1434.0   \n",
       "1            000100201  3  PE6     1.0   2.0   3.0     1.0  2019.0  1434.0   \n",
       "2            000102801  2  PE6     1.0  28.0   2.0     1.0  2019.0  1434.0   \n",
       "3            000102801  6  PE6     1.0  28.0   6.0     1.0  2019.0  1434.0   \n",
       "4            000104801  2  PE6     1.0  48.0   2.0     1.0  2019.0  1434.0   \n",
       "...                   ...  ...     ...   ...   ...     ...     ...     ...   \n",
       "38330        325406201  2  PE6  3254.0  62.0   2.0  3254.0  2019.0  1440.0   \n",
       "38331        325406301  2  PE6  3254.0  63.0   2.0  3254.0  2019.0  1440.0   \n",
       "38332        325407001  2  PE6  3254.0  70.0   2.0  3254.0  2019.0  1440.0   \n",
       "38333        325407201  2  PE6  3254.0  72.0   2.0  3254.0  2019.0  1440.0   \n",
       "38334        325407401  2  PE6  3254.0  74.0   2.0  3254.0  2019.0  1440.0   \n",
       "\n",
       "       V009    V010  ...  V102  V120  V121  V122  V123  V124  V125  V127  \\\n",
       "0       4.0  1986.0  ...   1.0   1.0   1.0   1.0   0.0   0.0   0.0  33.0   \n",
       "1       1.0  2007.0  ...   1.0   1.0   1.0   1.0   0.0   0.0   0.0  33.0   \n",
       "2       6.0  1983.0  ...   1.0   1.0   1.0   1.0   1.0   1.0   1.0  33.0   \n",
       "3       3.0  1970.0  ...   1.0   1.0   1.0   1.0   1.0   1.0   1.0  33.0   \n",
       "4       5.0  1991.0  ...   1.0   0.0   0.0   0.0   0.0   0.0   0.0  34.0   \n",
       "...     ...     ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "38330  12.0  1971.0  ...   2.0   0.0   0.0   0.0   0.0   0.0   0.0  96.0   \n",
       "38331   6.0  1988.0  ...   2.0   0.0   0.0   0.0   0.0   0.0   0.0  96.0   \n",
       "38332   7.0  1973.0  ...   2.0   0.0   0.0   0.0   0.0   0.0   0.0  96.0   \n",
       "38333  12.0  1994.0  ...   2.0   0.0   0.0   0.0   0.0   0.0   0.0  96.0   \n",
       "38334  10.0  1996.0  ...   2.0   0.0   0.0   0.0   0.0   0.0   0.0  96.0   \n",
       "\n",
       "       V133  year  \n",
       "0      16.0  2019  \n",
       "1       6.0  2019  \n",
       "2      16.0  2019  \n",
       "3       4.0  2019  \n",
       "4       1.0  2019  \n",
       "...     ...   ...  \n",
       "38330   4.0  2019  \n",
       "38331   6.0  2019  \n",
       "38332   3.0  2019  \n",
       "38333   9.0  2019  \n",
       "38334   9.0  2019  \n",
       "\n",
       "[38335 rows x 23 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create the new column\n",
    "rec1_1.loc[:,\"year\"]=\"2019\"\n",
    "rec1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We udpate the new variable\n",
    "new_var_labels1.update({'year':\"Year of the survey\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Merge `rec1_1`, `rec2_1`, and `rec3_1` using **CASEID**. Name this new object as `endes_2019`. **Hint: Use [this link](https://stackoverflow.com/questions/53645882/pandas-merging-101)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASEID</th>\n",
       "      <th>V000</th>\n",
       "      <th>V001</th>\n",
       "      <th>V002</th>\n",
       "      <th>V003</th>\n",
       "      <th>V004</th>\n",
       "      <th>V007</th>\n",
       "      <th>V008</th>\n",
       "      <th>V009</th>\n",
       "      <th>V010</th>\n",
       "      <th>...</th>\n",
       "      <th>V508</th>\n",
       "      <th>V509</th>\n",
       "      <th>V510</th>\n",
       "      <th>V511</th>\n",
       "      <th>V512</th>\n",
       "      <th>V513</th>\n",
       "      <th>V525</th>\n",
       "      <th>V613</th>\n",
       "      <th>V714</th>\n",
       "      <th>V715</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000100201  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000100201  3</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000102801  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000102801  6</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000104801  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38330</th>\n",
       "      <td>325406201  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38331</th>\n",
       "      <td>325406301  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>1236.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38332</th>\n",
       "      <td>325407001  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38333</th>\n",
       "      <td>325407201  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38334</th>\n",
       "      <td>325407401  2</td>\n",
       "      <td>PE6</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1362.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38335 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   CASEID V000    V001  V002  V003    V004    V007    V008  \\\n",
       "0            000100201  2  PE6     1.0   2.0   2.0     1.0  2019.0  1434.0   \n",
       "1            000100201  3  PE6     1.0   2.0   3.0     1.0  2019.0  1434.0   \n",
       "2            000102801  2  PE6     1.0  28.0   2.0     1.0  2019.0  1434.0   \n",
       "3            000102801  6  PE6     1.0  28.0   6.0     1.0  2019.0  1434.0   \n",
       "4            000104801  2  PE6     1.0  48.0   2.0     1.0  2019.0  1434.0   \n",
       "...                   ...  ...     ...   ...   ...     ...     ...     ...   \n",
       "38330        325406201  2  PE6  3254.0  62.0   2.0  3254.0  2019.0  1440.0   \n",
       "38331        325406301  2  PE6  3254.0  63.0   2.0  3254.0  2019.0  1440.0   \n",
       "38332        325407001  2  PE6  3254.0  70.0   2.0  3254.0  2019.0  1440.0   \n",
       "38333        325407201  2  PE6  3254.0  72.0   2.0  3254.0  2019.0  1440.0   \n",
       "38334        325407401  2  PE6  3254.0  74.0   2.0  3254.0  2019.0  1440.0   \n",
       "\n",
       "       V009    V010  ...    V508    V509  V510  V511  V512  V513  V525  V613  \\\n",
       "0       4.0  1986.0  ...  2008.0  1297.0   1.0  21.0  11.0   3.0  17.0   2.0   \n",
       "1       1.0  2007.0  ...     NaN     NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2       6.0  1983.0  ...  2011.0  1344.0   1.0  28.0   7.0   2.0  18.0   3.0   \n",
       "3       3.0  1970.0  ...  1984.0  1016.0   5.0  14.0  34.0   7.0  14.0   0.0   \n",
       "4       5.0  1991.0  ...  2009.0  1320.0   1.0  18.0   9.0   2.0  15.0   2.0   \n",
       "...     ...     ...  ...     ...     ...   ...   ...   ...   ...   ...   ...   \n",
       "38330  12.0  1971.0  ...  1985.0  1032.0   1.0  14.0  34.0   7.0  12.0   3.0   \n",
       "38331   6.0  1988.0  ...  2002.0  1236.0   1.0  14.0  17.0   4.0  12.0   3.0   \n",
       "38332   7.0  1973.0  ...  1986.0  1043.0   1.0  13.0  33.0   7.0  12.0   4.0   \n",
       "38333  12.0  1994.0  ...  2008.0  1301.0   1.0  13.0  11.0   3.0  13.0   2.0   \n",
       "38334  10.0  1996.0  ...  2013.0  1362.0   1.0  16.0   6.0   2.0  15.0   2.0   \n",
       "\n",
       "       V714  V715  \n",
       "0       1.0  11.0  \n",
       "1       NaN   NaN  \n",
       "2       1.0  14.0  \n",
       "3       0.0   6.0  \n",
       "4       0.0   6.0  \n",
       "...     ...   ...  \n",
       "38330   0.0   5.0  \n",
       "38331   0.0   7.0  \n",
       "38332   0.0   5.0  \n",
       "38333   0.0  11.0  \n",
       "38334   0.0  11.0  \n",
       "\n",
       "[38335 rows x 64 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We merge requested dataframes in one\n",
    "endes_2019=rec1_1.merge(rec2_1,on='CASEID',how='outer')\\\n",
    "                .merge(rec3_1,on='CASEID',how='outer')\n",
    "endes_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Unify all the `new_var_labels` in one object and `new_value_labels` in another one object. Name these two objects as `var_labels` and `value_labels`. Use them to generate new attributes for `endes_2019`. These attributes should be named as `var_labels` and `value_labels`. **Hint: Use `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We unify the variables \n",
    "var_labels = new_var_labels1.copy()\n",
    "var_labels.update(new_var_labels2)\n",
    "var_labels.update(new_var_labels3)\n",
    "\n",
    "# We unify the values\n",
    "value_labels = new_value_labels1.copy()\n",
    "value_labels.update(new_value_labels2)\n",
    "value_labels.update(new_value_labels3)\n",
    "\n",
    "# We generate new attributes for endes_2019\n",
    "endes_2019['var_labels'] = [var_labels] * len(endes_2019)\n",
    "endes_2019['value_labels'] = [value_labels] * len(endes_2019)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now, replicate your code of the prevoius sections but for years **2019, 2018, 2017, 2016, 2015**. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their **variables and values labels** from this path `\"../../_data/endes/\"`. For this excersie you must use a for loop. This loop must iterate over **2019, 2018, 2017, 2016, 2015 folders** and import these files. All the files have the same name. You must store these files and their labels in a nested dictionary named as `all_data`. The keys of the dictionary should be named as `year_2019`, for example, and the keys of the nested dictionary should be `data`, `var_labels`, and `value_labels`. **Hint: Use [this link](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=4d6de78e00e7001f16bf6473c2eb7ce24fb611cd&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f616c6578616e6465727175697370652f4469706c6f6d61646f5f505543502f346436646537386530306537303031663136626636343733633265623763653234666236313163642f4c6563747572655f342f4c6563747572655f342e6970796e62&logged_in=true&nwo=alexanderquispe%2FDiplomado_PUCP&path=Lecture_4%2FLecture_4.ipynb&platform=windows&repository_id=427747212&repository_type=Repository&version=95#4.2.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We Define the path to the data directory, the years and file names\n",
    "base_path = \"../../_data/endes/\"\n",
    "years = [2019, 2018, 2017, 2016, 2015]\n",
    "file_names = [\"REC0111.sav\", \"RE223132.sav\", \"RE516171.sav\"]\n",
    "\n",
    "# Initialize the nested dictionary\n",
    "all_data = {}\n",
    "\n",
    "# Loop over each year\n",
    "for year in years:\n",
    "    # Initialize a nested dictionary for the current year\n",
    "    year_data = {}\n",
    "    \n",
    "    # Loop over each file\n",
    "    for file_name in file_names:\n",
    "        # Construct the full file path\n",
    "        file_path = f\"{base_path}{year}/{file_name}\"\n",
    "        \n",
    "        # Read the .sav file and capture the data along with the metadata (labels)\n",
    "        data, meta = pyreadstat.read_sav(file_path)\n",
    "        \n",
    "        # Extract variable labels and value labels\n",
    "        var_labels = meta.column_labels\n",
    "        value_labels = meta.value_labels\n",
    "        \n",
    "        # Store the data and labels in the nested dictionary\n",
    "        year_data[file_name] = {\n",
    "            'data': data,\n",
    "            'var_labels': var_labels,\n",
    "            'value_labels': value_labels\n",
    "        }\n",
    "    \n",
    "    # Store the year's data in the all_data dictionary\n",
    "    all_data[f'year_{year}'] = year_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use `all_data` to append all the data sets. Store all data sets in a list using `for loop`. Then, use `pd.concat` to append all the data sets. Also, you must reset the index to have a good-looking data. This new object should be named as `endes_data_2015_2019`. **Hint: Use [this code](https://stackoverflow.com/questions/32444138/concatenate-a-list-of-pandas-dataframes-together)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store all dataFrames\n",
    "dataframes_list = []\n",
    "# Collect the dataFrames directly into a list\n",
    "dataframes_list = [\n",
    "    data_info['data'].assign(year=int(year.split('_')[1]))\n",
    "    for year, year_data in all_data.items()\n",
    "    for file_name, data_info in year_data.items()\n",
    "]\n",
    "\n",
    "# Concatenate all dataFrames\n",
    "endes_data_2015_2019 = pd.concat(dataframes_list, axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Store all the `var_labels` and `value_labels` in a dictionary named as `all_var_labels` and `all_value_labels`. The first keys should be the year for both dictionaries.Then, use them to generate new attributes for `endes_data_2015_2019`. These attributes should be named as `var_labels` and `value_labels`.  **Hint: Use [this link](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=4d6de78e00e7001f16bf6473c2eb7ce24fb611cd&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f616c6578616e6465727175697370652f4469706c6f6d61646f5f505543502f346436646537386530306537303031663136626636343733633265623763653234666236313163642f4c6563747572655f342f4c6563747572655f342e6970796e62&logged_in=true&nwo=alexanderquispe%2FDiplomado_PUCP&path=Lecture_4%2FLecture_4.ipynb&platform=windows&repository_id=427747212&repository_type=Repository&version=95#4.2.3.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionaries to store the labels\n",
    "all_var_labels = {}\n",
    "all_value_labels = {}\n",
    "\n",
    "# Fill the dictionaries with variable labels and values ​​by year\n",
    "for year_key, year_data in all_data.items():\n",
    "    year = int(year_key.split('_')[1])  # Extract the year from the key\n",
    "    all_var_labels[year] = {}\n",
    "    all_value_labels[year] = {}\n",
    "    \n",
    "    for file_name, file_data in year_data.items():\n",
    "        if isinstance(file_data['var_labels'], dict):\n",
    "            all_var_labels[year].update(file_data['var_labels'])\n",
    "        else:\n",
    "            print(f\"Unexpected type for var_labels in file {file_name} of year {year}\")\n",
    "        \n",
    "        if isinstance(file_data['value_labels'], dict):\n",
    "            all_value_labels[year].update(file_data['value_labels'])\n",
    "        else:\n",
    "            print(f\"Unexpected type for value_labels in file {file_name} of year {year}\")\n",
    "\n",
    "# Function to get labels based on year\n",
    "def get_labels_for_year(year, labels_dict):\n",
    "    return labels_dict.get(year, {})\n",
    "\n",
    "# Add new columns for var_labels and value_labels\n",
    "endes_data_2015_2019['var_labels'] = endes_data_2015_2019['year'].apply(lambda year: get_labels_for_year(year, all_var_labels))\n",
    "endes_data_2015_2019['value_labels'] = endes_data_2015_2019['year'].apply(lambda year: get_labels_for_year(year, all_value_labels))\n",
    "\n",
    "# Reset index for better display\n",
    "endes_data_2015_2019.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the resulting DataFrame to verify\n",
    "print(endes_data_2015_2019.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Use `endes_data_2015_2019` data to generate a new object named `mean_key_vars` to find the mean of **total children ever born (V201)**, **Ideal number of children (V613)**, **Husbands education-single yrs (V715)**, and **Age at first marriage (V511)** by year and department **(V024)**. Name these columns as **mean_total_children, mean_ideal_children, mean_hb_yr_educ and mean_first_marriage**, respectively. **Hint: Use groupby and [this link](https://stackoverflow.com/questions/40901770/is-there-a-simple-way-to-change-a-column-of-yes-no-to-1-0-in-a-pandas-dataframe).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Merge `mean_key_vars` with `endes_data_2015_2019`. Name this object `final_result`. **Hint: Use merge.**"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
